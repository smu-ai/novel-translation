{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c00f1b9c-135f-4d9f-a9a0-a9f15deee93f",
     "showTitle": false,
     "title": ""
    },
    "id": "IqM-T1RTzY6C"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
    "\n",
    "**[NEW] Llama-3 8b is trained on a crazy 15 trillion tokens! Llama-2 was 2 trillion.**\n",
    "\n",
    "Use our [Llama-3 8b Instruct](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) notebook for conversational style finetunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e057b3e-26de-43a9-a95c-0a4ae29eec56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b651ca45-58d6-43fe-836d-6fca220414ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workding dir: /home/inflaton/code/projects/courses/cs605/project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "workding_dir = str(Path.cwd().parent)\n",
    "os.chdir(workding_dir)\n",
    "sys.path.append(workding_dir)\n",
    "print(\"workding dir:\", workding_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0680175-fbd2-4021-be4f-05f1254aee01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading env vars from: /home/inflaton/code/projects/courses/cs605/project/.env\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "found_dotenv = find_dotenv(\".env\")\n",
    "\n",
    "if len(found_dotenv) == 0:\n",
    "    found_dotenv = find_dotenv(\".env.example\")\n",
    "print(f\"loading env vars from: {found_dotenv}\")\n",
    "load_dotenv(found_dotenv, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aa70875-4659-45a0-892d-b0d3e5a87528",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Qwen/Qwen2-1.5B-Instruct',\n",
       " True,\n",
       " 'models/Qwen2-1.5B-',\n",
       " 'Qwen2-1.5B-',\n",
       " 2048,\n",
       " 1,\n",
       " None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = os.getenv(\"MODEL_NAME\") or \"Qwen/Qwen2-7B\"\n",
    "token = os.getenv(\"HF_TOKEN\") or None\n",
    "load_in_4bit = os.getenv(\"LOAD_IN_4BIT\") == \"true\"\n",
    "local_model = os.getenv(\"LOCAL_MODEL\") \n",
    "hub_model = os.getenv(\"HUB_MODEL\") \n",
    "num_train_epochs = int(os.getenv(\"NUM_TRAIN_EPOCHS\") or 0)\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "\n",
    "\n",
    "model_name, load_in_4bit, local_model, hub_model, max_seq_length, num_train_epochs, dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b64444f2-0d1a-4903-af8b-add8940b0b0f",
     "showTitle": false,
     "title": ""
    },
    "id": "r2v_X2fA0Df5"
   },
   "source": [
    "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
    "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
    "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
    "* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.\n",
    "* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4242761f-16a1-4347-9971-387353388c1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c93f0b1c0274665974cbed7982d50fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea15a7a89d6b47239ee3db06865403b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec15a3936dc047cead627f6538fefd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e14d3f52184aeb9d865f218957f289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feb4b185d464ebebabf180804bb4efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c21469d5eb4fe4862d09bb2e02271e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdef7702e1b243b3a22831cd644c056c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caa24b68ac54a219f33a99f03851d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd89d5bd9a344b0caad54533d27652db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd10af106df34e42a949c7713ea51a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcc7728dab84b8b84f0840f118dc900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download(repo_id=model_name)\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e610e2fe-0f57-4f00-ba9d-82900c2efb55",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353,
     "referenced_widgets": [
      "98c58f23f4d549518832cb2d18f796e8",
      "09b76013aa9e45efb6deb23a7a0d0925",
      "39b29a75374b45c0a22506010be2b84e",
      "78e5400bff924a92a4cc61c4ff18b182",
      "2a58d04b428c46f4b3dbadd3bc6cd529",
      "dea41c5260884aa6879b5e1d1697b14f",
      "89965917796a4f81b899fdc7685f33df",
      "30cdc32298134cb0be4d41615b9e5774",
      "47928317548c454bba6358ab132e8dee",
      "b9b313fd861948f5aba25b24b1518d30",
      "4c666f4ace3943f8b80ecd20e7503236",
      "c22f71b1f85843209d7e5321506b9cb9",
      "1f44c9ce1adf470cbb19784493ed209f",
      "f1addc4479d849879e743cf9089e6540",
      "8b3505352a5a42bf910428c40ce40465",
      "4c4c88d4c701450692fa0f6b0c5764b0",
      "0c34be936c8145d3ab41282f30a70713",
      "0a92c56bfa134ef583220d7ef0b13e17",
      "43dec2ede91341f5af60eb522e18e984",
      "d8e5318cead340c4adbeaccc05d39225",
      "49277aeeac16434a865a4d12308b1abc",
      "2157f01726d748f8a9ae4a00664430da",
      "fce7a61c25ec4390af43d92b7c473a45",
      "30307300bc4e4baf96560e30969a82b6",
      "8fc142b628fb40568730234de1cafde2",
      "a8464a4c711e4e00aafdfc919b60d07e",
      "5f40db8173dd4d76b6ef5ed6d9ec8b6e",
      "e36a3f9eff0e4cf68834d66b0213ae96",
      "a0037bdccf254159becde630bee3d1db",
      "4ae7e449e4ea4c729b5f34607c18ebae",
      "3572201bd4d74a58b7a665f9bdfdcdba",
      "fb995c740590427b882572c81d4e848c",
      "201b59ccd9f845e197029b57e424aefc",
      "cf245afeb1c04f29a24d291608c3d157",
      "b518dcee69074b87be73957cd810e7ed",
      "e29104486d594b2992d7285e0ef77371",
      "6578fd7acdb54c4c93528ea431fd0144",
      "d35db8148a354c56aaac56dbae22536f",
      "d891f8d0b1fc462f8008d02bb2a15692",
      "cced8fd7e998472794f3f3e3018956a5",
      "a9f0cc51fc3d4d7b874c32dcf1c5bdf2",
      "2f6c70dd266c4816bfad3fd3d192929a",
      "370692d819df41828b48c4ad446f977b",
      "a0bf9160eb2647409b3200270914b90f",
      "2d18ddf6482c4d97829ac0e5a7b9868f",
      "9f679ad3ec7f4fe8ad0510ffb57bc2ab",
      "f2df530d22c74977b249dd9fb5f4829b",
      "89b2ef0dbfea47ab8e6f8d659e3351d1",
      "3056b148aa9f4e6e8aa3b61d26886255",
      "4ea63adfce694725bdba878aef709dd3",
      "74501720ac7e4dbb911a4a99b3633bc6",
      "21db8a77b00d4a4e82fdfa608657531f",
      "6dbbedeca9314e66ae50e44ffa31a414",
      "b8908fa0df3743ecb9d12983a739104f",
      "177c78fce95d4b4ab33057c5a048d693",
      "27155728b6b84cb199c91c940095d0a8",
      "6b91feeed5464877991ac2c207aebe7c",
      "cca8113c54c0495daedce1327bf9c68b",
      "2e63a29e2f7247bba5beede9a568c99f",
      "5c9d781c28944f3eb86e2a6d44efdf18",
      "4b2061b8a73c43ffb0c2f83daf0d0183",
      "69ac12aec0714318bf2c83d4f4e745f5",
      "e02f9b7849c64531835eb77b860d1c93",
      "56aee4853b7740e6a977254f5d1fa66d",
      "b993eaec6b224440bf80c0958c6fb536",
      "de868e26e7154f62aa86223a539ad421"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "a0e2d781-4934-415a-90b4-35165b9e44c5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n==((====))==  Unsloth: Fast Qwen2 patching release 2024.5\n   \\\\   /|    GPU: NVIDIA GeForce RTX 4080 Laptop GPU. Max memory: 11.994 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_dir,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=token,  # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38386613-6de7-4a25-adcc-afa177ec7b08",
     "showTitle": false,
     "title": ""
    },
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a134276-6131-49a2-a7c9-323bdae90c75",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "bc6d9ce7-f82a-4191-d0c5-ec8247d9b9eb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ab89c06-bdb5-415c-b942-70e918a75243",
     "showTitle": false,
     "title": ""
    },
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "\n",
    "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
    "\n",
    "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c7a53a4-bf84-4771-a8c9-4f6947e1bc9d",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "26e4202cca81496a90d15a0dd4ca9cf1",
      "ba90fdb8822d47dab7ba203bee297f37",
      "61560ff6a36b44f4a9dfdae5c52791d4",
      "95fbe66647904c06a20f640630d6dc0e",
      "57182a263d324a3dbf1471c74290a0d5",
      "0f8b6bfe16894500838793f2491d403f",
      "bb19f6c747754682a514373a3a0535ba",
      "db19fc8d37db4e45a5790a876836d8c4",
      "36166c7bcb854b34aca1f41a5d6ea50b",
      "b0a370dc20654b279b9680692e34418e",
      "cfeb365ddf7548d58b2557f22737fcf5",
      "73e352a3404f4c7dad0737f57d29e92f",
      "988a0e8c1f89446086858da0a891a79c",
      "4ccedf0d93094e63b57a0f8a434fba06",
      "6b2012c3f88547af8884a9ea90e3164b",
      "7e29cb8dd4df4d5b94407cd8fd3f2011",
      "ad2be500fc164c0f86f33e914ef8e6a0",
      "5234566b1bfc4655b8d582ea5b46ed9f",
      "4463edd481c1467f914c7dcd6c6e6ffc",
      "6d3b9a05db0b4dadb638c686faa0c40a",
      "938f45f1b3e24118b815d96ae34ba86a",
      "9367047a800747f79c6b225d92397846",
      "d1b47d39450d4019ae85c9b2f943eeaf",
      "4dcf6ff672d24983a1877a8431709aa9",
      "7975adbc2ec5489ea7fa0167e620d85c",
      "71ce208e20d6483abb9ed923510c86d7",
      "cfe8cae0e22b495bafa221a63d13b283",
      "5807d5fb827d490fb3bc698f801ffff5",
      "c4f2b06a82fd4987b8b659524a7b503b",
      "6e34619b45934040b6092e6fb01ea7fe",
      "271ddaa553a042d09b6db7b450643d8f",
      "d69dc491b3ab44d7852b21873ed7bb7f",
      "f401d53bf28e44eb906bce6c05412662",
      "daf4cd890b35422683d22fd30bc71e83",
      "b0240cd9a4554b29ae11f8051984a1c6",
      "bc883d4cf13e4f8b8a4fe5f410cb6efd",
      "99fdbb0300c14c139d1937c646f0cfe7",
      "c161d94df0f04feba9542237e0856c22",
      "edaf890370314a218f138015faa0b05d",
      "697f027529b54ee9956bae78a11e0611",
      "e9159e03e61f4f56978ece9c3bca49b2",
      "810ff6c0e17d4fa09a30fef27eacff90",
      "7358cdad832342c983e31efb8754ab78",
      "e9adf418296e436fb48bb9f78885598b"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "7e2045fb-9ce9-49b1-b6e7-d5c9bc92455c"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a61a6d-694f-431a-b38d-6e607a5b6055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction', 'text'],\n",
       "    num_rows: 51760\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f53167c3-9cfc-4c28-975b-7453e6d659d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give three tips for staying healthy.\n------------------------\n\n------------------------\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dataset[\"instruction\"][0],\n",
    "    dataset[\"input\"][0],\n",
    "    dataset[\"output\"][0],\n",
    "    sep=\"\\n------------------------\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce178255-bd99-4d6d-a499-7389d4372e5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "51760"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "392a1fb9-8390-4fc4-8f7c-ed3b1536c07d",
     "showTitle": false,
     "title": ""
    },
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f205bd1-94cc-423d-a90f-cea0c1c4dde4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "3cf2dd993b5e4d3daecf61e4bab5a404",
      "087b76a8b7514269b1f0ab29b062e444",
      "35b0e8c26d6640e9bd0ed7b242a423d8",
      "54ad89e05fd74576b9b8b5b5a10eaf8d",
      "a41dc44766444a998bec2d777f249d23",
      "a069d2ab23824f29aa320ac256e2cfe9",
      "06e806c82c7b4cbea31c5358dd9c3434",
      "2e5087c76f98437cb5dc729230358cba",
      "036fc5746f43416db18c19ad8fd36677",
      "fdb1941405ed4e4aa06019933892deb3",
      "668d5377ca56426a99753867e6e24862"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "bce9db22-b022-4e43-de3f-c7ea4c9c3c4e"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989709505df34257aa64fa117a5d5e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=100,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b9a51c5-127f-41cc-b280-a90c52fd06e4",
     "showTitle": false,
     "title": ""
    },
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "c73d8dfa-f4a1-4a01-a6dc-018bf82516a2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4080 Laptop GPU. Max memory = 11.994 GB.\n1.604 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd274171-2f96-4df5-9331-43ae6b64757f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "69117b9b-b6f8-4d0e-c262-6998ba2c46bd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 6,470\n \"-____-\"     Number of trainable parameters = 18,464,768\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6470' max='6470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6470/6470 3:21:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.997900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.998400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.987100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ad3bc59-d97e-4cd6-bdd3-cf5605638689",
     "showTitle": false,
     "title": ""
    },
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "98f78253-86cf-4673-ff2b-923460c2b3fd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097.4857 seconds used for training.\n201.62 minutes used for training.\nPeak reserved memory = 3.688 GB.\nPeak reserved memory for training = 2.084 GB.\nPeak reserved memory % of max memory = 30.749 %.\nPeak reserved memory for training % of max memory = 17.375 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07c3727-729f-4164-9211-516eeedc2e2f",
     "showTitle": false,
     "title": ""
    },
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0932c69-9720-487d-b907-aa9ee391591a",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "b4333694-a541-4748-ac1c-e4118c161981"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nContinue the fibonnaci sequence.\n\n### Input:\n1, 1, 2, 3, 5, 8\n\n### Response:\nThe next number in the Fibonacci sequence after 8 is 13. The Fibonacci sequence is defined as follows: each number in the sequence is the sum of the two preceding ones, starting from 0 and 1.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"Continue the fibonnaci sequence.\",  # instruction\n",
    "            \"1, 1, 2, 3, 5, 8\",  # input\n",
    "            \"\",  # output - leave this blank for generation!\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "result = tokenizer.batch_decode(outputs)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0458cbe-a83e-4fe9-87aa-8ea7bbd52aa1",
     "showTitle": false,
     "title": ""
    },
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eef1f87-0cc1-46ef-b5a2-65dd78efa03e",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "13c0aed9-eb9c-4875-8776-209d93947551"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nContinue the fibonnaci sequence.\n\n### Input:\n1, 1, 2, 3, 5, 8\n\n### Response:\nThe next number in the Fibonacci sequence would be 13 (which is also known as the 6th term). The sequence continues as follows: 8, 13, 21, 34, 55, 89, and so on.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"Continue the fibonnaci sequence.\",  # instruction\n",
    "            \"1, 1, 2, 3, 5, 8\",  # input\n",
    "            \"\",  # output - leave this blank for generation!\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ad4c2ff-8999-4982-8aba-463289c8d348",
     "showTitle": false,
     "title": ""
    },
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e55fd07-f291-4e07-bf2b-1dec6673dc25",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "bdd5b069-e944-4c81-8094-468999c210ec"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/inflaton/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8 - will assume that the vocabulary was not modified.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('models/Qwen2-1.5B-lora/tokenizer_config.json',\n",
       " 'models/Qwen2-1.5B-lora/special_tokens_map.json',\n",
       " 'models/Qwen2-1.5B-lora/vocab.json',\n",
       " 'models/Qwen2-1.5B-lora/merges.txt',\n",
       " 'models/Qwen2-1.5B-lora/added_tokens.json',\n",
       " 'models/Qwen2-1.5B-lora/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_lora_model = local_model + \"lora\"\n",
    "model.save_pretrained(local_lora_model)  # Local saving\n",
    "tokenizer.save_pretrained(local_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95dca414-7509-4528-a7ec-1e5e80fba7c3",
     "showTitle": false,
     "title": ""
    },
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e6a0cc-c9f9-484c-b8d9-8d8f3bf61c38",
     "showTitle": false,
     "title": ""
    },
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b85e727d-db41-488b-be92-0ab2bfaab2e1",
     "showTitle": false,
     "title": ""
    },
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        local_lora_model,  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f11b56e-4eee-40af-8c40-7ffdf2d126fc",
     "showTitle": false,
     "title": ""
    },
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e250342-69ce-4b98-bf2c-7a0897cb339f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, save_method, publish=True):\n",
    "    model.save_pretrained_merged(\n",
    "        local_model + save_method,\n",
    "        tokenizer,\n",
    "        save_method=save_method,\n",
    "    )\n",
    "\n",
    "    if publish:\n",
    "        model.push_to_hub_merged(\n",
    "            hub_model + save_method,\n",
    "            tokenizer,\n",
    "            save_method=save_method,\n",
    "            token=token,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e816774b-666a-4d0f-ad30-d39d824ad4c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... Done.\nUnsloth: Saving LoRA adapters. Please wait...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7566e3e5142d4de59484cc1d730823c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/73.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9e525c0bb24389a2b72ccd57689d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lora model to https://huggingface.co/Qwen2-1.5B-lora\n"
     ]
    }
   ],
   "source": [
    "save_model(model, tokenizer, \"lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a657a2d-3dd7-410d-97f1-58f2da1afdd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 4bit...\nThis might take 5 minutes...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\nUnsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 10 minutes for Llama-7b... Done.\nUnsloth: Merging 4bit and LoRA weights to 4bit...\nThis might take 5 minutes...\nDone.\nUnsloth: Saving 4bit Bitsandbytes model. Please wait...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b8bdbee589440f87173b79eb2f437f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9b142712514289a3a9215239fbe661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged_4bit model to https://huggingface.co/Qwen2-1.5B-merged_4bit_forced\n"
     ]
    }
   ],
   "source": [
    "save_model(model, tokenizer, \"merged_4bit_forced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2bce0a-3eda-45d5-8011-127f8d1ce9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 22.73 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 34.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 22.66 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 303.53it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving to organization with address inflaton/Qwen2-1.5B-merged_16bit\nUnsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nUnsloth: Saving to organization with address inflaton/Qwen2-1.5B-merged_16bit\nUnsloth: Uploading all files... Please wait...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c3d063920a4e9788e41fe9e726e3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee423b5cb94c40b0ae49355e8e75a4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/727M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fdb693feeb46c2a2c0790ea58e3feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\nSaved merged model to https://huggingface.co/None/Qwen2-1.5B-merged_16bit\n"
     ]
    }
   ],
   "source": [
    "save_model(model, tokenizer, \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab66a30-07a6-4bc0-8e22-0301fc86f234",
     "showTitle": false,
     "title": ""
    },
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22685d0f-4a92-446b-9efb-697f48b0be99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_model_gguf(model, tokenizer, quantization_method, publish=True):\n",
    "    model.save_pretrained_gguf(\n",
    "        local_model + quantization_method,\n",
    "        tokenizer,\n",
    "        quantization_method=quantization_method,\n",
    "    )\n",
    "\n",
    "    if publish:\n",
    "        model.push_to_hub_gguf(\n",
    "            hub_model + \"gguf-\" + quantization_method,\n",
    "            tokenizer,\n",
    "            quantization_method=quantization_method,\n",
    "            token=token,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e321a34a-d763-44f2-a876-196f9f30cfe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 25.7 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 224.54it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting qwen2 model. Can use fast conversion = False.\nUnsloth: We must use f16 for non Llama and Mistral models.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to f16 will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at models/Qwen2-1.5B-f16 into f16 GGUF format.\nThe output location will be ./models/Qwen2-1.5B-f16-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-f16\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'models/Qwen2-1.5B-f16-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.15G/3.09G [00:04<00:02, 431Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:08<00:00, 372Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'models/Qwen2-1.5B-f16-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-f16-unsloth.F16.gguf\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 25.7 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 302.94it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to f16 will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at Qwen2-1.5B-gguf-f16 into f16 GGUF format.\nThe output location will be ./Qwen2-1.5B-gguf-f16-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-gguf-f16\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'Qwen2-1.5B-gguf-f16-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.15G/3.09G [00:04<00:02, 447Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:06<00:00, 461Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'Qwen2-1.5B-gguf-f16-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-gguf-f16-unsloth.F16.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a6d824b988413a83e9b0b537b5fed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2-1.5B-gguf-f16-unsloth.F16.gguf:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/inflaton/Qwen2-1.5B-gguf-f16\n"
     ]
    }
   ],
   "source": [
    "save_model_gguf(model, tokenizer, quantization_method=\"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b91b3b92-3955-415b-8f01-6a6cc634f591",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 25.7 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 221.69it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q8_0 will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at models/Qwen2-1.5B-q8_0 into f16 GGUF format.\nThe output location will be ./models/Qwen2-1.5B-q8_0-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-q8_0\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'models/Qwen2-1.5B-q8_0-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.12G/3.09G [00:04<00:02, 449Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 436Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'models/Qwen2-1.5B-q8_0-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-q8_0-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './models/Qwen2-1.5B-q8_0-unsloth.F16.gguf' to './models/Qwen2-1.5B-q8_0-unsloth.Q8_0.gguf' as Q8_0 using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./models/Qwen2-1.5B-q8_0-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-q8_0\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q8_0 .. size =   445.12 MiB ->   236.47 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  19/ 254]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  20/ 254]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  21/ 254]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  22/ 254]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  23/ 254]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  24/ 254]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  25/ 254]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  26/ 254]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  27/ 254]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  28/ 254]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  29/ 254]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  30/ 254]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  31/ 254]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  32/ 254]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  33/ 254]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  34/ 254]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  35/ 254]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  36/ 254]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  37/ 254]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  38/ 254]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  39/ 254]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  40/ 254]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  41/ 254]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  42/ 254]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  43/ 254]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  44/ 254]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  45/ 254]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  46/ 254]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  47/ 254]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  48/ 254]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  49/ 254]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  50/ 254]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  51/ 254]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  52/ 254]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  53/ 254]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  54/ 254]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  55/ 254]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  56/ 254]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  57/ 254]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  58/ 254]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  59/ 254]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  60/ 254]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  61/ 254]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  62/ 254]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  63/ 254]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  64/ 254]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  65/ 254]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  66/ 254]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  67/ 254]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  68/ 254]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  69/ 254]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  70/ 254]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  71/ 254]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  72/ 254]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  73/ 254]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  74/ 254]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  75/ 254]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  76/ 254]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  77/ 254]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  78/ 254]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  79/ 254]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  80/ 254]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  81/ 254]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  82/ 254]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  83/ 254]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  84/ 254]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  85/ 254]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  86/ 254]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  87/ 254]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  88/ 254]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  89/ 254]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  90/ 254]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  91/ 254]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  92/ 254]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  93/ 254]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  94/ 254]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  95/ 254]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  96/ 254]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  97/ 254]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  98/ 254]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  99/ 254]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 100/ 254]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 101/ 254]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 102/ 254]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 103/ 254]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 104/ 254]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 105/ 254]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 106/ 254]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 107/ 254]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 108/ 254]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 109/ 254]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 110/ 254]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 111/ 254]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 112/ 254]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 113/ 254]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 114/ 254]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 115/ 254]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 116/ 254]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 117/ 254]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 118/ 254]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 119/ 254]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 120/ 254]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 121/ 254]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 122/ 254]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 123/ 254]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 124/ 254]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 125/ 254]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 126/ 254]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 127/ 254]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 128/ 254]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 129/ 254]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 130/ 254]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 131/ 254]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 132/ 254]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 133/ 254]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 134/ 254]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 135/ 254]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 136/ 254]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 137/ 254]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 138/ 254]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 139/ 254]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 140/ 254]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 141/ 254]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 142/ 254]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 143/ 254]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 144/ 254]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 145/ 254]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 146/ 254]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 147/ 254]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 148/ 254]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 149/ 254]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 150/ 254]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 151/ 254]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 152/ 254]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 153/ 254]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 154/ 254]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 155/ 254]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 156/ 254]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 157/ 254]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 158/ 254]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 159/ 254]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 160/ 254]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 161/ 254]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 162/ 254]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 163/ 254]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\nggml_validate_row_data: found inf value at block 213000\nllama_model_quantize: failed to quantize: tensor 'blk.3.attn_k.weight' has invalid data\nmain: failed to quantize model from './models/Qwen2-1.5B-q8_0-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-q8_0-unsloth.Q8_0.gguf\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 25.68 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 112.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q8_0 will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at Qwen2-1.5B-gguf-q8_0 into f16 GGUF format.\nThe output location will be ./Qwen2-1.5B-gguf-q8_0-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-gguf-q8_0\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'Qwen2-1.5B-gguf-q8_0-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.15G/3.09G [00:04<00:02, 453Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:06<00:00, 452Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'Qwen2-1.5B-gguf-q8_0-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-gguf-q8_0-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './Qwen2-1.5B-gguf-q8_0-unsloth.F16.gguf' to './Qwen2-1.5B-gguf-q8_0-unsloth.Q8_0.gguf' as Q8_0 using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./Qwen2-1.5B-gguf-q8_0-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-gguf-q8_0\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q8_0 .. size =   445.12 MiB ->   236.47 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  19/ 254]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  20/ 254]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  21/ 254]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  22/ 254]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  23/ 254]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  24/ 254]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  25/ 254]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  26/ 254]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  27/ 254]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  28/ 254]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  29/ 254]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  30/ 254]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  31/ 254]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  32/ 254]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  33/ 254]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  34/ 254]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  35/ 254]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  36/ 254]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  37/ 254]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  38/ 254]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  39/ 254]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  40/ 254]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  41/ 254]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  42/ 254]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  43/ 254]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  44/ 254]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  45/ 254]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  46/ 254]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  47/ 254]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  48/ 254]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  49/ 254]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  50/ 254]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  51/ 254]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  52/ 254]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  53/ 254]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  54/ 254]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  55/ 254]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  56/ 254]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  57/ 254]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  58/ 254]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  59/ 254]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  60/ 254]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  61/ 254]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  62/ 254]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  63/ 254]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  64/ 254]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  65/ 254]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  66/ 254]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  67/ 254]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  68/ 254]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  69/ 254]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  70/ 254]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  71/ 254]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  72/ 254]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  73/ 254]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  74/ 254]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  75/ 254]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  76/ 254]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  77/ 254]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  78/ 254]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  79/ 254]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  80/ 254]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  81/ 254]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  82/ 254]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  83/ 254]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  84/ 254]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  85/ 254]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  86/ 254]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  87/ 254]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  88/ 254]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  89/ 254]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  90/ 254]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  91/ 254]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  92/ 254]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  93/ 254]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  94/ 254]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  95/ 254]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  96/ 254]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  97/ 254]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  98/ 254]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  99/ 254]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 100/ 254]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 101/ 254]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 102/ 254]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 103/ 254]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 104/ 254]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 105/ 254]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 106/ 254]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 107/ 254]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 108/ 254]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 109/ 254]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 110/ 254]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 111/ 254]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 112/ 254]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 113/ 254]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 114/ 254]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 115/ 254]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 116/ 254]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 117/ 254]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 118/ 254]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 119/ 254]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 120/ 254]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 121/ 254]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 122/ 254]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 123/ 254]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 124/ 254]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 125/ 254]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 126/ 254]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 127/ 254]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 128/ 254]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 129/ 254]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 130/ 254]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 131/ 254]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 132/ 254]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 133/ 254]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 134/ 254]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 135/ 254]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 136/ 254]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 137/ 254]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 138/ 254]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 139/ 254]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 140/ 254]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 141/ 254]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 142/ 254]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 143/ 254]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 144/ 254]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 145/ 254]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 146/ 254]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 147/ 254]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 148/ 254]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 149/ 254]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 150/ 254]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 151/ 254]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 152/ 254]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 153/ 254]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 154/ 254]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 155/ 254]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 156/ 254]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 157/ 254]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[ 158/ 254]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[ 159/ 254]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 160/ 254]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 161/ 254]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 162/ 254]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[ 163/ 254]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\nggml_validate_row_data: found inf value at block 213000\nllama_model_quantize: failed to quantize: tensor 'blk.3.attn_k.weight' has invalid data\nmain: failed to quantize model from './Qwen2-1.5B-gguf-q8_0-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-gguf-q8_0-unsloth.Q8_0.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783db07275394788af92464b102232eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2-1.5B-gguf-q8_0-unsloth.Q8_0.gguf:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/inflaton/Qwen2-1.5B-gguf-q8_0\n"
     ]
    }
   ],
   "source": [
    "save_model_gguf(model, tokenizer, quantization_method=\"q8_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153a0c90-b342-40cd-860a-b30253b656e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 25.69 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 324.28it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q5_k_m will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at models/Qwen2-1.5B-q5_k_m into f16 GGUF format.\nThe output location will be ./models/Qwen2-1.5B-q5_k_m-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-q5_k_m\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'models/Qwen2-1.5B-q5_k_m-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.15G/3.09G [00:04<00:02, 415Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 429Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'models/Qwen2-1.5B-q5_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-q5_k_m-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q5_k_m. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './models/Qwen2-1.5B-q5_k_m-unsloth.F16.gguf' to './models/Qwen2-1.5B-q5_k_m-unsloth.Q5_K_M.gguf' as Q5_K_M using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./models/Qwen2-1.5B-q5_k_m-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-q5_k_m\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  19/ 254]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  20/ 254]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  21/ 254]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  22/ 254]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  23/ 254]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  24/ 254]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  25/ 254]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  26/ 254]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  27/ 254]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  28/ 254]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  29/ 254]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  30/ 254]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  31/ 254]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  32/ 254]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  33/ 254]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  34/ 254]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  35/ 254]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  36/ 254]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  37/ 254]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  38/ 254]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  39/ 254]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  40/ 254]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  41/ 254]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  42/ 254]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  43/ 254]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  44/ 254]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  45/ 254]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  46/ 254]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  47/ 254]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  48/ 254]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  49/ 254]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  50/ 254]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  51/ 254]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  52/ 254]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  53/ 254]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  54/ 254]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  55/ 254]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  56/ 254]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  57/ 254]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  58/ 254]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  59/ 254]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  60/ 254]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  61/ 254]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  62/ 254]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  63/ 254]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  64/ 254]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  65/ 254]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  66/ 254]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  67/ 254]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  68/ 254]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  69/ 254]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  70/ 254]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  71/ 254]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  72/ 254]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  73/ 254]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  74/ 254]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  75/ 254]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  76/ 254]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  77/ 254]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  78/ 254]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  79/ 254]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  80/ 254]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  81/ 254]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  82/ 254]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  83/ 254]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  84/ 254]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  85/ 254]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  86/ 254]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  87/ 254]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  88/ 254]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  89/ 254]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  90/ 254]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  91/ 254]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  92/ 254]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  93/ 254]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  94/ 254]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  95/ 254]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  96/ 254]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  97/ 254]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  98/ 254]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  99/ 254]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 100/ 254]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 101/ 254]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 102/ 254]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 103/ 254]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 104/ 254]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 105/ 254]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 106/ 254]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 107/ 254]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 108/ 254]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 109/ 254]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 110/ 254]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 111/ 254]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 112/ 254]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 113/ 254]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 114/ 254]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 115/ 254]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 116/ 254]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 117/ 254]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 118/ 254]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 119/ 254]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 120/ 254]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 121/ 254]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 122/ 254]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 123/ 254]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 124/ 254]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 125/ 254]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 126/ 254]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 127/ 254]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 128/ 254]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 129/ 254]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 130/ 254]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 131/ 254]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 132/ 254]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 133/ 254]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 134/ 254]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 135/ 254]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 136/ 254]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 137/ 254]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 138/ 254]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 139/ 254]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 140/ 254]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 141/ 254]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 142/ 254]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 143/ 254]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 144/ 254]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 145/ 254]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 146/ 254]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 147/ 254]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 148/ 254]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 149/ 254]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 150/ 254]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 151/ 254]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 152/ 254]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 153/ 254]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 154/ 254]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 155/ 254]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 156/ 254]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 157/ 254]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 158/ 254]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 159/ 254]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 160/ 254]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 161/ 254]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 162/ 254]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 163/ 254]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\nggml_validate_row_data: found inf value at block 213000\nllama_model_quantize: failed to quantize: tensor 'blk.3.attn_k.weight' has invalid data\nmain: failed to quantize model from './models/Qwen2-1.5B-q5_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-q5_k_m-unsloth.Q5_K_M.gguf\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 25.68 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 156.65it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q5_k_m will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at Qwen2-1.5B-gguf-q5_k_m into f16 GGUF format.\nThe output location will be ./Qwen2-1.5B-gguf-q5_k_m-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-gguf-q5_k_m\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'Qwen2-1.5B-gguf-q5_k_m-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.15G/3.09G [00:04<00:02, 452Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:06<00:00, 456Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'Qwen2-1.5B-gguf-q5_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-gguf-q5_k_m-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q5_k_m. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './Qwen2-1.5B-gguf-q5_k_m-unsloth.F16.gguf' to './Qwen2-1.5B-gguf-q5_k_m-unsloth.Q5_K_M.gguf' as Q5_K_M using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./Qwen2-1.5B-gguf-q5_k_m-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-gguf-q5_k_m\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  19/ 254]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  20/ 254]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  21/ 254]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  22/ 254]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  23/ 254]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  24/ 254]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  25/ 254]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  26/ 254]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  27/ 254]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  28/ 254]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  29/ 254]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  30/ 254]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  31/ 254]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  32/ 254]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  33/ 254]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  34/ 254]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  35/ 254]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  36/ 254]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  37/ 254]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  38/ 254]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  39/ 254]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  40/ 254]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  41/ 254]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  42/ 254]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  43/ 254]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  44/ 254]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  45/ 254]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  46/ 254]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  47/ 254]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  48/ 254]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  49/ 254]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  50/ 254]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  51/ 254]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  52/ 254]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  53/ 254]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  54/ 254]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  55/ 254]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  56/ 254]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  57/ 254]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  58/ 254]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  59/ 254]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  60/ 254]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  61/ 254]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  62/ 254]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  63/ 254]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  64/ 254]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  65/ 254]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  66/ 254]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  67/ 254]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  68/ 254]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  69/ 254]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  70/ 254]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  71/ 254]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  72/ 254]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  73/ 254]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  74/ 254]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  75/ 254]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  76/ 254]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  77/ 254]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  78/ 254]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  79/ 254]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  80/ 254]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  81/ 254]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  82/ 254]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  83/ 254]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  84/ 254]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  85/ 254]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  86/ 254]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  87/ 254]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  88/ 254]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  89/ 254]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  90/ 254]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  91/ 254]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  92/ 254]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  93/ 254]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  94/ 254]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  95/ 254]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  96/ 254]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  97/ 254]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  98/ 254]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  99/ 254]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 100/ 254]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 101/ 254]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 102/ 254]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 103/ 254]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 104/ 254]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 105/ 254]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 106/ 254]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 107/ 254]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 108/ 254]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 109/ 254]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 110/ 254]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 111/ 254]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 112/ 254]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 113/ 254]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 114/ 254]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 115/ 254]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 116/ 254]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 117/ 254]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 118/ 254]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 119/ 254]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 120/ 254]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 121/ 254]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 122/ 254]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 123/ 254]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 124/ 254]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 125/ 254]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 126/ 254]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 127/ 254]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 128/ 254]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 129/ 254]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 130/ 254]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 131/ 254]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 132/ 254]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 133/ 254]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 134/ 254]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 135/ 254]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 136/ 254]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 137/ 254]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 138/ 254]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 139/ 254]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 140/ 254]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 141/ 254]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 142/ 254]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 143/ 254]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 144/ 254]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 145/ 254]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 146/ 254]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 147/ 254]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 148/ 254]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 149/ 254]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 150/ 254]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 151/ 254]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 152/ 254]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 153/ 254]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 154/ 254]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 155/ 254]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[ 156/ 254]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 157/ 254]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[ 158/ 254]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 159/ 254]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 160/ 254]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 161/ 254]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 162/ 254]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[ 163/ 254]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\nggml_validate_row_data: found inf value at block 213000\nllama_model_quantize: failed to quantize: tensor 'blk.3.attn_k.weight' has invalid data\nmain: failed to quantize model from './Qwen2-1.5B-gguf-q5_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-gguf-q5_k_m-unsloth.Q5_K_M.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa448fe7e8844eea1aebdfd7f96ee2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2-1.5B-gguf-q5_k_m-unsloth.Q5_K_M.gguf:   0%|          | 0.00/792M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/inflaton/Qwen2-1.5B-gguf-q5_k_m\n"
     ]
    }
   ],
   "source": [
    "save_model_gguf(model, tokenizer, quantization_method=\"q5_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5f01642-6e3e-441f-890c-a62564564fd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 25.68 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 305.91it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at models/Qwen2-1.5B-q4_k_m into f16 GGUF format.\nThe output location will be ./models/Qwen2-1.5B-q4_k_m-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-q4_k_m\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'models/Qwen2-1.5B-q4_k_m-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.15G/3.09G [00:04<00:02, 427Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:06<00:00, 443Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'models/Qwen2-1.5B-q4_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-q4_k_m-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './models/Qwen2-1.5B-q4_k_m-unsloth.F16.gguf' to './models/Qwen2-1.5B-q4_k_m-unsloth.Q4_K_M.gguf' as Q4_K_M using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./models/Qwen2-1.5B-q4_k_m-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-q4_k_m\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  19/ 254]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  20/ 254]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  21/ 254]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  22/ 254]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  23/ 254]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  24/ 254]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  25/ 254]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  26/ 254]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  27/ 254]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  28/ 254]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  29/ 254]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  30/ 254]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  31/ 254]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  32/ 254]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  33/ 254]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  34/ 254]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  35/ 254]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  36/ 254]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  37/ 254]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  38/ 254]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  39/ 254]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  40/ 254]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  41/ 254]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  42/ 254]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  43/ 254]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  44/ 254]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  45/ 254]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  46/ 254]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  47/ 254]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  48/ 254]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  49/ 254]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  50/ 254]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  51/ 254]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  52/ 254]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  53/ 254]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  54/ 254]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  55/ 254]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  56/ 254]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  57/ 254]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  58/ 254]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  59/ 254]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  60/ 254]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  61/ 254]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  62/ 254]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  63/ 254]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  64/ 254]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  65/ 254]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  66/ 254]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  67/ 254]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  68/ 254]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  69/ 254]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  70/ 254]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  71/ 254]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  72/ 254]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  73/ 254]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  74/ 254]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  75/ 254]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  76/ 254]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  77/ 254]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  78/ 254]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  79/ 254]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  80/ 254]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  81/ 254]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  82/ 254]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  83/ 254]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  84/ 254]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  85/ 254]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  86/ 254]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  87/ 254]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  88/ 254]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  89/ 254]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  90/ 254]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  91/ 254]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  92/ 254]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  93/ 254]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  94/ 254]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  95/ 254]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  96/ 254]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  97/ 254]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  98/ 254]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  99/ 254]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 100/ 254]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 101/ 254]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 102/ 254]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 103/ 254]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 104/ 254]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 105/ 254]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 106/ 254]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 107/ 254]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 108/ 254]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 109/ 254]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 110/ 254]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 111/ 254]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 112/ 254]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 113/ 254]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 114/ 254]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 115/ 254]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 116/ 254]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 117/ 254]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 118/ 254]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 119/ 254]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 120/ 254]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 121/ 254]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 122/ 254]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 123/ 254]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 124/ 254]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 125/ 254]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 126/ 254]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 127/ 254]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 128/ 254]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 129/ 254]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 130/ 254]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 131/ 254]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 132/ 254]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 133/ 254]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 134/ 254]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 135/ 254]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 136/ 254]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 137/ 254]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 138/ 254]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 139/ 254]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 140/ 254]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 141/ 254]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 142/ 254]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 143/ 254]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 144/ 254]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 145/ 254]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 146/ 254]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 147/ 254]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 148/ 254]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 149/ 254]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 150/ 254]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 151/ 254]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 152/ 254]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 153/ 254]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 154/ 254]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 155/ 254]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 156/ 254]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 157/ 254]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 158/ 254]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 159/ 254]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 160/ 254]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 161/ 254]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 162/ 254]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 163/ 254]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\nggml_validate_row_data: found inf value at block 213000\nllama_model_quantize: failed to quantize: tensor 'blk.3.attn_k.weight' has invalid data\nmain: failed to quantize model from './models/Qwen2-1.5B-q4_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-q4_k_m-unsloth.Q4_K_M.gguf\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 25.67 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 300.14it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at Qwen2-1.5B-gguf-q4_k_m into f16 GGUF format.\nThe output location will be ./Qwen2-1.5B-gguf-q4_k_m-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-gguf-q4_k_m\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'Qwen2-1.5B-gguf-q4_k_m-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.15G/3.09G [00:04<00:02, 322Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 424Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'Qwen2-1.5B-gguf-q4_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-gguf-q4_k_m-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './Qwen2-1.5B-gguf-q4_k_m-unsloth.F16.gguf' to './Qwen2-1.5B-gguf-q4_k_m-unsloth.Q4_K_M.gguf' as Q4_K_M using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./Qwen2-1.5B-gguf-q4_k_m-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-gguf-q4_k_m\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  19/ 254]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  20/ 254]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  21/ 254]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  22/ 254]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  23/ 254]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  24/ 254]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  25/ 254]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  26/ 254]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  27/ 254]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  28/ 254]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  29/ 254]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  30/ 254]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  31/ 254]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  32/ 254]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  33/ 254]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  34/ 254]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  35/ 254]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  36/ 254]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  37/ 254]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  38/ 254]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  39/ 254]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  40/ 254]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  41/ 254]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  42/ 254]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  43/ 254]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  44/ 254]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  45/ 254]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  46/ 254]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  47/ 254]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  48/ 254]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  49/ 254]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  50/ 254]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  51/ 254]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  52/ 254]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  53/ 254]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  54/ 254]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  55/ 254]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  56/ 254]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  57/ 254]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  58/ 254]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  59/ 254]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  60/ 254]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  61/ 254]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  62/ 254]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  63/ 254]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  64/ 254]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  65/ 254]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  66/ 254]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  67/ 254]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  68/ 254]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  69/ 254]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  70/ 254]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  71/ 254]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  72/ 254]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  73/ 254]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  74/ 254]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  75/ 254]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  76/ 254]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  77/ 254]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  78/ 254]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  79/ 254]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  80/ 254]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  81/ 254]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  82/ 254]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  83/ 254]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  84/ 254]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  85/ 254]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  86/ 254]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  87/ 254]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  88/ 254]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  89/ 254]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  90/ 254]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  91/ 254]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  92/ 254]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  93/ 254]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  94/ 254]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  95/ 254]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  96/ 254]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  97/ 254]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  98/ 254]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  99/ 254]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 100/ 254]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 101/ 254]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 102/ 254]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 103/ 254]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 104/ 254]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 105/ 254]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 106/ 254]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 107/ 254]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 108/ 254]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 109/ 254]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 110/ 254]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 111/ 254]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 112/ 254]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 113/ 254]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 114/ 254]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 115/ 254]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 116/ 254]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 117/ 254]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 118/ 254]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 119/ 254]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 120/ 254]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 121/ 254]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 122/ 254]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 123/ 254]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 124/ 254]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 125/ 254]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 126/ 254]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 127/ 254]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 128/ 254]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 129/ 254]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 130/ 254]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 131/ 254]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 132/ 254]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 133/ 254]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 134/ 254]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 135/ 254]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 136/ 254]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 137/ 254]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 138/ 254]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 139/ 254]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 140/ 254]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 141/ 254]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 142/ 254]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 143/ 254]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 144/ 254]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 145/ 254]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 146/ 254]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 147/ 254]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 148/ 254]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 149/ 254]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 150/ 254]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 151/ 254]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 152/ 254]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 153/ 254]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 154/ 254]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 155/ 254]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[ 156/ 254]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 157/ 254]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[ 158/ 254]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[ 159/ 254]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[ 160/ 254]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[ 161/ 254]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 162/ 254]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[ 163/ 254]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\nggml_validate_row_data: found inf value at block 213000\nllama_model_quantize: failed to quantize: tensor 'blk.3.attn_k.weight' has invalid data\nmain: failed to quantize model from './Qwen2-1.5B-gguf-q4_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-gguf-q4_k_m-unsloth.Q4_K_M.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53df92e70c69424ab530dc86d86a7a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2-1.5B-gguf-q4_k_m-unsloth.Q4_K_M.gguf:   0%|          | 0.00/701M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/inflaton/Qwen2-1.5B-gguf-q4_k_m\n"
     ]
    }
   ],
   "source": [
    "save_model_gguf(model, tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6743cc94-d9be-4d75-8467-74c9467f3db6",
     "showTitle": false,
     "title": ""
    },
    "id": "bDp0zNpwe6U_"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db83551-2fad-400b-b554-ce5de22a2e2a",
     "showTitle": false,
     "title": ""
    },
    "id": "Zt9CHJqO6p30"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
    "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
    "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
    "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
    "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
    "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
    "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
    "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
    "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01_Alpaca_+_Qwen2-1.5B_Unsloth_2x_faster_finetuning",
   "widgets": {}
  },
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "036fc5746f43416db18c19ad8fd36677": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06e806c82c7b4cbea31c5358dd9c3434": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "087b76a8b7514269b1f0ab29b062e444": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a069d2ab23824f29aa320ac256e2cfe9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_06e806c82c7b4cbea31c5358dd9c3434",
      "value": "Map‚Äá(num_proc=2):‚Äá100%"
     }
    },
    "09b76013aa9e45efb6deb23a7a0d0925": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dea41c5260884aa6879b5e1d1697b14f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_89965917796a4f81b899fdc7685f33df",
      "value": "config.json:‚Äá100%"
     }
    },
    "0a92c56bfa134ef583220d7ef0b13e17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c34be936c8145d3ab41282f30a70713": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f8b6bfe16894500838793f2491d403f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "177c78fce95d4b4ab33057c5a048d693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f44c9ce1adf470cbb19784493ed209f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c34be936c8145d3ab41282f30a70713",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0a92c56bfa134ef583220d7ef0b13e17",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "201b59ccd9f845e197029b57e424aefc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2157f01726d748f8a9ae4a00664430da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21db8a77b00d4a4e82fdfa608657531f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26e4202cca81496a90d15a0dd4ca9cf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba90fdb8822d47dab7ba203bee297f37",
       "IPY_MODEL_61560ff6a36b44f4a9dfdae5c52791d4",
       "IPY_MODEL_95fbe66647904c06a20f640630d6dc0e"
      ],
      "layout": "IPY_MODEL_57182a263d324a3dbf1471c74290a0d5"
     }
    },
    "27155728b6b84cb199c91c940095d0a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b91feeed5464877991ac2c207aebe7c",
       "IPY_MODEL_cca8113c54c0495daedce1327bf9c68b",
       "IPY_MODEL_2e63a29e2f7247bba5beede9a568c99f"
      ],
      "layout": "IPY_MODEL_5c9d781c28944f3eb86e2a6d44efdf18"
     }
    },
    "271ddaa553a042d09b6db7b450643d8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a58d04b428c46f4b3dbadd3bc6cd529": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d18ddf6482c4d97829ac0e5a7b9868f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f679ad3ec7f4fe8ad0510ffb57bc2ab",
       "IPY_MODEL_f2df530d22c74977b249dd9fb5f4829b",
       "IPY_MODEL_89b2ef0dbfea47ab8e6f8d659e3351d1"
      ],
      "layout": "IPY_MODEL_3056b148aa9f4e6e8aa3b61d26886255"
     }
    },
    "2e5087c76f98437cb5dc729230358cba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e63a29e2f7247bba5beede9a568c99f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b993eaec6b224440bf80c0958c6fb536",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_de868e26e7154f62aa86223a539ad421",
      "value": "‚Äá464/464‚Äá[00:00&lt;00:00,‚Äá27.1kB/s]"
     }
    },
    "2f6c70dd266c4816bfad3fd3d192929a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30307300bc4e4baf96560e30969a82b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e36a3f9eff0e4cf68834d66b0213ae96",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a0037bdccf254159becde630bee3d1db",
      "value": "generation_config.json:‚Äá100%"
     }
    },
    "3056b148aa9f4e6e8aa3b61d26886255": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30cdc32298134cb0be4d41615b9e5774": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3572201bd4d74a58b7a665f9bdfdcdba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "35b0e8c26d6640e9bd0ed7b242a423d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e5087c76f98437cb5dc729230358cba",
      "max": 51760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_036fc5746f43416db18c19ad8fd36677",
      "value": 51760
     }
    },
    "36166c7bcb854b34aca1f41a5d6ea50b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "370692d819df41828b48c4ad446f977b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39b29a75374b45c0a22506010be2b84e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30cdc32298134cb0be4d41615b9e5774",
      "max": 1179,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47928317548c454bba6358ab132e8dee",
      "value": 1179
     }
    },
    "3cf2dd993b5e4d3daecf61e4bab5a404": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_087b76a8b7514269b1f0ab29b062e444",
       "IPY_MODEL_35b0e8c26d6640e9bd0ed7b242a423d8",
       "IPY_MODEL_54ad89e05fd74576b9b8b5b5a10eaf8d"
      ],
      "layout": "IPY_MODEL_a41dc44766444a998bec2d777f249d23"
     }
    },
    "43dec2ede91341f5af60eb522e18e984": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4463edd481c1467f914c7dcd6c6e6ffc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47928317548c454bba6358ab132e8dee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49277aeeac16434a865a4d12308b1abc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ae7e449e4ea4c729b5f34607c18ebae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b2061b8a73c43ffb0c2f83daf0d0183": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c4c88d4c701450692fa0f6b0c5764b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c666f4ace3943f8b80ecd20e7503236": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ccedf0d93094e63b57a0f8a434fba06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4463edd481c1467f914c7dcd6c6e6ffc",
      "max": 44307561,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d3b9a05db0b4dadb638c686faa0c40a",
      "value": 44307561
     }
    },
    "4dcf6ff672d24983a1877a8431709aa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5807d5fb827d490fb3bc698f801ffff5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c4f2b06a82fd4987b8b659524a7b503b",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
     }
    },
    "4ea63adfce694725bdba878aef709dd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5234566b1bfc4655b8d582ea5b46ed9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54ad89e05fd74576b9b8b5b5a10eaf8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdb1941405ed4e4aa06019933892deb3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_668d5377ca56426a99753867e6e24862",
      "value": "‚Äá51760/51760‚Äá[01:02&lt;00:00,‚Äá1131.51‚Äáexamples/s]"
     }
    },
    "56aee4853b7740e6a977254f5d1fa66d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57182a263d324a3dbf1471c74290a0d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5807d5fb827d490fb3bc698f801ffff5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c9d781c28944f3eb86e2a6d44efdf18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f40db8173dd4d76b6ef5ed6d9ec8b6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61560ff6a36b44f4a9dfdae5c52791d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db19fc8d37db4e45a5790a876836d8c4",
      "max": 11610,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36166c7bcb854b34aca1f41a5d6ea50b",
      "value": 11610
     }
    },
    "6578fd7acdb54c4c93528ea431fd0144": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_370692d819df41828b48c4ad446f977b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a0bf9160eb2647409b3200270914b90f",
      "value": "‚Äá50.6k/50.6k‚Äá[00:00&lt;00:00,‚Äá2.71MB/s]"
     }
    },
    "668d5377ca56426a99753867e6e24862": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "697f027529b54ee9956bae78a11e0611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69ac12aec0714318bf2c83d4f4e745f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b2012c3f88547af8884a9ea90e3164b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_938f45f1b3e24118b815d96ae34ba86a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9367047a800747f79c6b225d92397846",
      "value": "‚Äá44.3M/44.3M‚Äá[00:01&lt;00:00,‚Äá31.0MB/s]"
     }
    },
    "6b91feeed5464877991ac2c207aebe7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b2061b8a73c43ffb0c2f83daf0d0183",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_69ac12aec0714318bf2c83d4f4e745f5",
      "value": "special_tokens_map.json:‚Äá100%"
     }
    },
    "6d3b9a05db0b4dadb638c686faa0c40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6dbbedeca9314e66ae50e44ffa31a414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6e34619b45934040b6092e6fb01ea7fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71ce208e20d6483abb9ed923510c86d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d69dc491b3ab44d7852b21873ed7bb7f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f401d53bf28e44eb906bce6c05412662",
      "value": "‚Äá51760/51760‚Äá[00:01&lt;00:00,‚Äá45512.81‚Äáexamples/s]"
     }
    },
    "7358cdad832342c983e31efb8754ab78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e352a3404f4c7dad0737f57d29e92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_988a0e8c1f89446086858da0a891a79c",
       "IPY_MODEL_4ccedf0d93094e63b57a0f8a434fba06",
       "IPY_MODEL_6b2012c3f88547af8884a9ea90e3164b"
      ],
      "layout": "IPY_MODEL_7e29cb8dd4df4d5b94407cd8fd3f2011"
     }
    },
    "74501720ac7e4dbb911a4a99b3633bc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78e5400bff924a92a4cc61c4ff18b182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9b313fd861948f5aba25b24b1518d30",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4c666f4ace3943f8b80ecd20e7503236",
      "value": "‚Äá1.18k/1.18k‚Äá[00:00&lt;00:00,‚Äá31.3kB/s]"
     }
    },
    "7975adbc2ec5489ea7fa0167e620d85c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e34619b45934040b6092e6fb01ea7fe",
      "max": 51760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_271ddaa553a042d09b6db7b450643d8f",
      "value": 51760
     }
    },
    "7e29cb8dd4df4d5b94407cd8fd3f2011": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "810ff6c0e17d4fa09a30fef27eacff90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "89965917796a4f81b899fdc7685f33df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89b2ef0dbfea47ab8e6f8d659e3351d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8908fa0df3743ecb9d12983a739104f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_177c78fce95d4b4ab33057c5a048d693",
      "value": "‚Äá9.09M/9.09M‚Äá[00:00&lt;00:00,‚Äá32.6MB/s]"
     }
    },
    "8b3505352a5a42bf910428c40ce40465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49277aeeac16434a865a4d12308b1abc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2157f01726d748f8a9ae4a00664430da",
      "value": "‚Äá5.70G/5.70G‚Äá[01:02&lt;00:00,‚Äá30.1MB/s]"
     }
    },
    "8fc142b628fb40568730234de1cafde2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ae7e449e4ea4c729b5f34607c18ebae",
      "max": 172,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3572201bd4d74a58b7a665f9bdfdcdba",
      "value": 172
     }
    },
    "9367047a800747f79c6b225d92397846": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "938f45f1b3e24118b815d96ae34ba86a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95fbe66647904c06a20f640630d6dc0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0a370dc20654b279b9680692e34418e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cfeb365ddf7548d58b2557f22737fcf5",
      "value": "‚Äá11.6k/11.6k‚Äá[00:00&lt;00:00,‚Äá716kB/s]"
     }
    },
    "988a0e8c1f89446086858da0a891a79c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad2be500fc164c0f86f33e914ef8e6a0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5234566b1bfc4655b8d582ea5b46ed9f",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "98c58f23f4d549518832cb2d18f796e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09b76013aa9e45efb6deb23a7a0d0925",
       "IPY_MODEL_39b29a75374b45c0a22506010be2b84e",
       "IPY_MODEL_78e5400bff924a92a4cc61c4ff18b182"
      ],
      "layout": "IPY_MODEL_2a58d04b428c46f4b3dbadd3bc6cd529"
     }
    },
    "99fdbb0300c14c139d1937c646f0cfe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7358cdad832342c983e31efb8754ab78",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e9adf418296e436fb48bb9f78885598b",
      "value": "‚Äá51760/51760‚Äá[00:01&lt;00:00,‚Äá38665.95‚Äáexamples/s]"
     }
    },
    "9f679ad3ec7f4fe8ad0510ffb57bc2ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ea63adfce694725bdba878aef709dd3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_74501720ac7e4dbb911a4a99b3633bc6",
      "value": "tokenizer.json:‚Äá100%"
     }
    },
    "a0037bdccf254159becde630bee3d1db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a069d2ab23824f29aa320ac256e2cfe9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0bf9160eb2647409b3200270914b90f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a41dc44766444a998bec2d777f249d23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8464a4c711e4e00aafdfc919b60d07e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb995c740590427b882572c81d4e848c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_201b59ccd9f845e197029b57e424aefc",
      "value": "‚Äá172/172‚Äá[00:00&lt;00:00,‚Äá12.0kB/s]"
     }
    },
    "a9f0cc51fc3d4d7b874c32dcf1c5bdf2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad2be500fc164c0f86f33e914ef8e6a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0240cd9a4554b29ae11f8051984a1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edaf890370314a218f138015faa0b05d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_697f027529b54ee9956bae78a11e0611",
      "value": "Map:‚Äá100%"
     }
    },
    "b0a370dc20654b279b9680692e34418e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b518dcee69074b87be73957cd810e7ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d891f8d0b1fc462f8008d02bb2a15692",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cced8fd7e998472794f3f3e3018956a5",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "b8908fa0df3743ecb9d12983a739104f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b993eaec6b224440bf80c0958c6fb536": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9b313fd861948f5aba25b24b1518d30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba90fdb8822d47dab7ba203bee297f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f8b6bfe16894500838793f2491d403f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bb19f6c747754682a514373a3a0535ba",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "bb19f6c747754682a514373a3a0535ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc883d4cf13e4f8b8a4fe5f410cb6efd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9159e03e61f4f56978ece9c3bca49b2",
      "max": 51760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_810ff6c0e17d4fa09a30fef27eacff90",
      "value": 51760
     }
    },
    "c161d94df0f04feba9542237e0856c22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c22f71b1f85843209d7e5321506b9cb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f44c9ce1adf470cbb19784493ed209f",
       "IPY_MODEL_f1addc4479d849879e743cf9089e6540",
       "IPY_MODEL_8b3505352a5a42bf910428c40ce40465"
      ],
      "layout": "IPY_MODEL_4c4c88d4c701450692fa0f6b0c5764b0"
     }
    },
    "c4f2b06a82fd4987b8b659524a7b503b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cca8113c54c0495daedce1327bf9c68b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e02f9b7849c64531835eb77b860d1c93",
      "max": 464,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_56aee4853b7740e6a977254f5d1fa66d",
      "value": 464
     }
    },
    "cced8fd7e998472794f3f3e3018956a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf245afeb1c04f29a24d291608c3d157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b518dcee69074b87be73957cd810e7ed",
       "IPY_MODEL_e29104486d594b2992d7285e0ef77371",
       "IPY_MODEL_6578fd7acdb54c4c93528ea431fd0144"
      ],
      "layout": "IPY_MODEL_d35db8148a354c56aaac56dbae22536f"
     }
    },
    "cfe8cae0e22b495bafa221a63d13b283": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfeb365ddf7548d58b2557f22737fcf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1b47d39450d4019ae85c9b2f943eeaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dcf6ff672d24983a1877a8431709aa9",
       "IPY_MODEL_7975adbc2ec5489ea7fa0167e620d85c",
       "IPY_MODEL_71ce208e20d6483abb9ed923510c86d7"
      ],
      "layout": "IPY_MODEL_cfe8cae0e22b495bafa221a63d13b283"
     }
    },
    "d35db8148a354c56aaac56dbae22536f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d69dc491b3ab44d7852b21873ed7bb7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d891f8d0b1fc462f8008d02bb2a15692": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8e5318cead340c4adbeaccc05d39225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "daf4cd890b35422683d22fd30bc71e83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0240cd9a4554b29ae11f8051984a1c6",
       "IPY_MODEL_bc883d4cf13e4f8b8a4fe5f410cb6efd",
       "IPY_MODEL_99fdbb0300c14c139d1937c646f0cfe7"
      ],
      "layout": "IPY_MODEL_c161d94df0f04feba9542237e0856c22"
     }
    },
    "db19fc8d37db4e45a5790a876836d8c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de868e26e7154f62aa86223a539ad421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dea41c5260884aa6879b5e1d1697b14f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e02f9b7849c64531835eb77b860d1c93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e29104486d594b2992d7285e0ef77371": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9f0cc51fc3d4d7b874c32dcf1c5bdf2",
      "max": 50641,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2f6c70dd266c4816bfad3fd3d192929a",
      "value": 50641
     }
    },
    "e36a3f9eff0e4cf68834d66b0213ae96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9159e03e61f4f56978ece9c3bca49b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9adf418296e436fb48bb9f78885598b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "edaf890370314a218f138015faa0b05d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1addc4479d849879e743cf9089e6540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43dec2ede91341f5af60eb522e18e984",
      "max": 5702746405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d8e5318cead340c4adbeaccc05d39225",
      "value": 5702746405
     }
    },
    "f2df530d22c74977b249dd9fb5f4829b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21db8a77b00d4a4e82fdfa608657531f",
      "max": 9085698,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6dbbedeca9314e66ae50e44ffa31a414",
      "value": 9085698
     }
    },
    "f401d53bf28e44eb906bce6c05412662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb995c740590427b882572c81d4e848c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fce7a61c25ec4390af43d92b7c473a45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_30307300bc4e4baf96560e30969a82b6",
       "IPY_MODEL_8fc142b628fb40568730234de1cafde2",
       "IPY_MODEL_a8464a4c711e4e00aafdfc919b60d07e"
      ],
      "layout": "IPY_MODEL_5f40db8173dd4d76b6ef5ed6d9ec8b6e"
     }
    },
    "fdb1941405ed4e4aa06019933892deb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
