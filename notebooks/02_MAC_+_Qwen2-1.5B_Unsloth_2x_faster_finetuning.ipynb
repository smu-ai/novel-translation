{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e541d30d-9b99-4ac8-9b2f-a92970ec99ac",
     "showTitle": false,
     "title": ""
    },
    "id": "IqM-T1RTzY6C"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
    "\n",
    "**[NEW] Llama-3 8b is trained on a crazy 15 trillion tokens! Llama-2 was 2 trillion.**\n",
    "\n",
    "Use our [Llama-3 8b Instruct](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) notebook for conversational style finetunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdb846a8-bb92-435b-a140-132b409b3eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4c7c48e-486b-49d2-8a46-3d452cfba3ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workding dir: /home/inflaton/code/projects/courses/cs605/project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "workding_dir = str(Path.cwd().parent)\n",
    "os.chdir(workding_dir)\n",
    "sys.path.append(workding_dir)\n",
    "print(\"workding dir:\", workding_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c40f7af-1d44-4b0b-a95c-a16ad04751e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading env vars from: /home/inflaton/code/projects/courses/cs605/project/.env\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "found_dotenv = find_dotenv(\".env\")\n",
    "\n",
    "if len(found_dotenv) == 0:\n",
    "    found_dotenv = find_dotenv(\".env.example\")\n",
    "print(f\"loading env vars from: {found_dotenv}\")\n",
    "load_dotenv(found_dotenv, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6842a26-7b55-4d37-b08c-eb231c64dab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('inflaton/Qwen2-1.5B-merged_4bit_forced',\n",
       " True,\n",
       " 'models/Qwen2-1.5B-MAC-',\n",
       " 'Qwen2-1.5B-MAC-',\n",
       " 2048,\n",
       " 10,\n",
       " None,\n",
       " 'datasets/mac/mac.tsv')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = os.getenv(\"MODEL_NAME\") or \"Qwen/Qwen2-7B\"\n",
    "token = os.getenv(\"HF_TOKEN\") or None\n",
    "load_in_4bit = os.getenv(\"LOAD_IN_4BIT\") == \"true\"\n",
    "local_model = os.getenv(\"LOCAL_MODEL\") \n",
    "hub_model = os.getenv(\"HUB_MODEL\") \n",
    "num_train_epochs = int(os.getenv(\"NUM_TRAIN_EPOCHS\") or 0)\n",
    "data_path = os.getenv(\"DATA_PATH\")\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "\n",
    "\n",
    "model_name, load_in_4bit, local_model, hub_model, max_seq_length, num_train_epochs, dtype, data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f727c45-74c5-42fd-9247-3dde42949819",
     "showTitle": false,
     "title": ""
    },
    "id": "r2v_X2fA0Df5"
   },
   "source": [
    "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
    "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
    "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
    "* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.\n",
    "* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79093bc1-3085-498e-b7ba-673eec03f4d8",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353,
     "referenced_widgets": [
      "98c58f23f4d549518832cb2d18f796e8",
      "09b76013aa9e45efb6deb23a7a0d0925",
      "39b29a75374b45c0a22506010be2b84e",
      "78e5400bff924a92a4cc61c4ff18b182",
      "2a58d04b428c46f4b3dbadd3bc6cd529",
      "dea41c5260884aa6879b5e1d1697b14f",
      "89965917796a4f81b899fdc7685f33df",
      "30cdc32298134cb0be4d41615b9e5774",
      "47928317548c454bba6358ab132e8dee",
      "b9b313fd861948f5aba25b24b1518d30",
      "4c666f4ace3943f8b80ecd20e7503236",
      "c22f71b1f85843209d7e5321506b9cb9",
      "1f44c9ce1adf470cbb19784493ed209f",
      "f1addc4479d849879e743cf9089e6540",
      "8b3505352a5a42bf910428c40ce40465",
      "4c4c88d4c701450692fa0f6b0c5764b0",
      "0c34be936c8145d3ab41282f30a70713",
      "0a92c56bfa134ef583220d7ef0b13e17",
      "43dec2ede91341f5af60eb522e18e984",
      "d8e5318cead340c4adbeaccc05d39225",
      "49277aeeac16434a865a4d12308b1abc",
      "2157f01726d748f8a9ae4a00664430da",
      "fce7a61c25ec4390af43d92b7c473a45",
      "30307300bc4e4baf96560e30969a82b6",
      "8fc142b628fb40568730234de1cafde2",
      "a8464a4c711e4e00aafdfc919b60d07e",
      "5f40db8173dd4d76b6ef5ed6d9ec8b6e",
      "e36a3f9eff0e4cf68834d66b0213ae96",
      "a0037bdccf254159becde630bee3d1db",
      "4ae7e449e4ea4c729b5f34607c18ebae",
      "3572201bd4d74a58b7a665f9bdfdcdba",
      "fb995c740590427b882572c81d4e848c",
      "201b59ccd9f845e197029b57e424aefc",
      "cf245afeb1c04f29a24d291608c3d157",
      "b518dcee69074b87be73957cd810e7ed",
      "e29104486d594b2992d7285e0ef77371",
      "6578fd7acdb54c4c93528ea431fd0144",
      "d35db8148a354c56aaac56dbae22536f",
      "d891f8d0b1fc462f8008d02bb2a15692",
      "cced8fd7e998472794f3f3e3018956a5",
      "a9f0cc51fc3d4d7b874c32dcf1c5bdf2",
      "2f6c70dd266c4816bfad3fd3d192929a",
      "370692d819df41828b48c4ad446f977b",
      "a0bf9160eb2647409b3200270914b90f",
      "2d18ddf6482c4d97829ac0e5a7b9868f",
      "9f679ad3ec7f4fe8ad0510ffb57bc2ab",
      "f2df530d22c74977b249dd9fb5f4829b",
      "89b2ef0dbfea47ab8e6f8d659e3351d1",
      "3056b148aa9f4e6e8aa3b61d26886255",
      "4ea63adfce694725bdba878aef709dd3",
      "74501720ac7e4dbb911a4a99b3633bc6",
      "21db8a77b00d4a4e82fdfa608657531f",
      "6dbbedeca9314e66ae50e44ffa31a414",
      "b8908fa0df3743ecb9d12983a739104f",
      "177c78fce95d4b4ab33057c5a048d693",
      "27155728b6b84cb199c91c940095d0a8",
      "6b91feeed5464877991ac2c207aebe7c",
      "cca8113c54c0495daedce1327bf9c68b",
      "2e63a29e2f7247bba5beede9a568c99f",
      "5c9d781c28944f3eb86e2a6d44efdf18",
      "4b2061b8a73c43ffb0c2f83daf0d0183",
      "69ac12aec0714318bf2c83d4f4e745f5",
      "e02f9b7849c64531835eb77b860d1c93",
      "56aee4853b7740e6a977254f5d1fa66d",
      "b993eaec6b224440bf80c0958c6fb536",
      "de868e26e7154f62aa86223a539ad421"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "a0e2d781-4934-415a-90b4-35165b9e44c5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n==((====))==  Unsloth: Fast Qwen2 patching release 2024.5\n   \\\\   /|    GPU: NVIDIA GeForce RTX 4080 Laptop GPU. Max memory: 11.994 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 s, sys: 3.89 s, total: 17.2 s\nWall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    token=token,  # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "595f9fe5-f988-427b-bdf9-891c865e4831",
     "showTitle": false,
     "title": ""
    },
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2955ec04-6a3e-40db-94f0-88ca33939096",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "bc6d9ce7-f82a-4191-d0c5-ec8247d9b9eb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.48 s, sys: 0 ns, total: 4.48 s\nWall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c5de66-32c9-4b4b-8ab8-1aa3f3936fe8",
     "showTitle": false,
     "title": ""
    },
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "\n",
    "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
    "\n",
    "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec83a024-1f15-49dc-baf5-fdcf3568e058",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating train/test data files\n5723\n5661\nloading train/test data files\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aeb1857b1c41659de6b92853d999b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec62a451da340d599a02e4a4fc12e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chinese', 'english'],\n",
       "        num_rows: 4528\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chinese', 'english'],\n",
       "        num_rows: 1133\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_data_file = data_path.replace(\".tsv\", \"-train.tsv\")\n",
    "test_data_file = data_path.replace(\".tsv\", \"-test.tsv\")\n",
    "\n",
    "if not os.path.exists(train_data_file):\n",
    "    print(\"generating train/test data files\")\n",
    "    dataset = load_dataset(\"csv\", data_files=data_path, delimiter=\"\\t\", split=\"train\")\n",
    "    print(len(dataset))\n",
    "    dataset = dataset.filter(lambda x: x[\"chinese\"] and x[\"english\"])\n",
    "\n",
    "    datasets = dataset.train_test_split(test_size=0.2)\n",
    "    print(len(dataset))\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    train_df = pd.DataFrame(datasets[\"train\"])\n",
    "    test_df = pd.DataFrame(datasets[\"test\"])\n",
    "\n",
    "    # Save to TSV\n",
    "    train_df.to_csv(train_data_file, sep=\"\\t\", index=False)\n",
    "    test_df.to_csv(test_data_file, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "print(\"loading train/test data files\")\n",
    "datasets = load_dataset(\n",
    "    \"csv\", data_files={\"train\": train_data_file, \"test\": test_data_file}, delimiter=\"\\t\"\n",
    ")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf09db2e-daa2-449f-80b1-23cd9c8c4bff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chinese', 'english'],\n",
       "        num_rows: 4528\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chinese', 'english'],\n",
       "        num_rows: 1133\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7edf35a-f1c1-4d01-8b2c-1117b05cbf01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'chinese': 'ÂÖ®‰ªóÁùÄÁãê‰ªôÊê≠Êïë„ÄÇ', 'english': 'Because I was protected by a fox fairy.'},\n",
       " {'chinese': 'ËÄÅËÄøÁ´ØËµ∑Êû™ÔºåÁúØÁºùËµ∑‰∏ÄÂè™‰∏âËßíÁúºÔºå‰∏ÄÊêÇÊâ≥Êú∫Âìç‰∫ÜÊû™ÔºåÂÜ∞ÈõπËà¨ÁöÑÈáëÈ∫ªÈõÄÂäàÂì©Âï™Âï¶ÂæÄ‰∏ãËêΩÔºåÈìÅÁ†ÇÂ≠êÂú®Êü≥ÊûùÈó¥È£ûËø∏ÁùÄÔºåÂöìÂöìÊúâÂ£∞„ÄÇ',\n",
       "  'english': 'Old Geng picked up his shotgun, squinted, and pulled the trigger. Two sparrows crashed to the ground like hailstones as shotgun pellets tore noisily through the branches.'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0], datasets[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc8ae3f8-0e05-46d5-a89a-d8e269aab52b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'chinese': 'Âë®ÁëûÂÆ∂ÁöÑÈÅìÔºö‚ÄúÂ§™Â§™ËØ¥Ôºö‚Äò‰ªñ‰ª¨Âéü‰∏çÊòØ‰∏ÄÂÆ∂Â≠êÔºõ ÂΩìÂπ¥‰ªñ‰ª¨ÁöÑÁ•ñÂíåÂ§™ËÄÅÁà∑Âú®‰∏ÄÂ§ÑÂÅöÂÆòÔºåÂõ†Ëøû‰∫ÜÂÆóÁöÑ„ÄÇ',\n",
       "  'english': \"'She said they don't really belong to the family but were adopted into the clan years ago when your grandfather and theirs were working in the same office.\"},\n",
       " {'chinese': '‚ÄúÂê¨Âà∞‰∫ÜÂêóÔºü', 'english': \"'Did you hear that?'\"})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][1000], datasets[\"test\"][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25acb899-c16c-4eb4-ab56-3659f9e1bf71",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "26e4202cca81496a90d15a0dd4ca9cf1",
      "ba90fdb8822d47dab7ba203bee297f37",
      "61560ff6a36b44f4a9dfdae5c52791d4",
      "95fbe66647904c06a20f640630d6dc0e",
      "57182a263d324a3dbf1471c74290a0d5",
      "0f8b6bfe16894500838793f2491d403f",
      "bb19f6c747754682a514373a3a0535ba",
      "db19fc8d37db4e45a5790a876836d8c4",
      "36166c7bcb854b34aca1f41a5d6ea50b",
      "b0a370dc20654b279b9680692e34418e",
      "cfeb365ddf7548d58b2557f22737fcf5",
      "73e352a3404f4c7dad0737f57d29e92f",
      "988a0e8c1f89446086858da0a891a79c",
      "4ccedf0d93094e63b57a0f8a434fba06",
      "6b2012c3f88547af8884a9ea90e3164b",
      "7e29cb8dd4df4d5b94407cd8fd3f2011",
      "ad2be500fc164c0f86f33e914ef8e6a0",
      "5234566b1bfc4655b8d582ea5b46ed9f",
      "4463edd481c1467f914c7dcd6c6e6ffc",
      "6d3b9a05db0b4dadb638c686faa0c40a",
      "938f45f1b3e24118b815d96ae34ba86a",
      "9367047a800747f79c6b225d92397846",
      "d1b47d39450d4019ae85c9b2f943eeaf",
      "4dcf6ff672d24983a1877a8431709aa9",
      "7975adbc2ec5489ea7fa0167e620d85c",
      "71ce208e20d6483abb9ed923510c86d7",
      "cfe8cae0e22b495bafa221a63d13b283",
      "5807d5fb827d490fb3bc698f801ffff5",
      "c4f2b06a82fd4987b8b659524a7b503b",
      "6e34619b45934040b6092e6fb01ea7fe",
      "271ddaa553a042d09b6db7b450643d8f",
      "d69dc491b3ab44d7852b21873ed7bb7f",
      "f401d53bf28e44eb906bce6c05412662",
      "daf4cd890b35422683d22fd30bc71e83",
      "b0240cd9a4554b29ae11f8051984a1c6",
      "bc883d4cf13e4f8b8a4fe5f410cb6efd",
      "99fdbb0300c14c139d1937c646f0cfe7",
      "c161d94df0f04feba9542237e0856c22",
      "edaf890370314a218f138015faa0b05d",
      "697f027529b54ee9956bae78a11e0611",
      "e9159e03e61f4f56978ece9c3bca49b2",
      "810ff6c0e17d4fa09a30fef27eacff90",
      "7358cdad832342c983e31efb8754ab78",
      "e9adf418296e436fb48bb9f78885598b"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "7e2045fb-9ce9-49b1-b6e7-d5c9bc92455c"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Translate from Chinese to English.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"chinese\"]\n",
    "    outputs = examples[\"english\"]\n",
    "\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a0db5ed-7da2-4d80-b5ec-de75f57c9304",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8568b304604fa3989e8e548563020a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets[\"train\"].map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d88228e-1f49-4bec-a9e8-26578b345332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chinese', 'english', 'text'],\n",
       "    num_rows: 4528\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b24166de-b68c-4398-99ab-b8a7ae7e9a86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "def test_model(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    _ = model.generate(\n",
    "        **inputs, max_new_tokens=128, streamer=text_streamer, use_cache=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d73e0b3-c3d7-4fab-9b67-e31cdfc2dc2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nTranslate from Chinese to English.\n\n### Input:\nÂÖ®‰ªóÁùÄÁãê‰ªôÊê≠Êïë„ÄÇ\n\n### Response:\nBecause I was protected by a fox fairy.<|im_end|>\n<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "test_model(model, tokenizer, dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d61414ab-4b46-4fad-bdcf-d1b3fb61266f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because I was protected by a fox fairy.\n------------------------\nÂÖ®‰ªóÁùÄÁãê‰ªôÊê≠Êïë„ÄÇ\n------------------------\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nTranslate from Chinese to English.\n\n### Input:\nÂÖ®‰ªóÁùÄÁãê‰ªôÊê≠Êïë„ÄÇ\n\n### Response:\nBecause I was protected by a fox fairy.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\n",
    "    dataset[\"english\"][0],\n",
    "    dataset[\"chinese\"][0],\n",
    "    dataset[\"text\"][0],\n",
    "    sep=\"\\n------------------------\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22d59771-aff7-4a70-b51c-6b2c63878f62",
     "showTitle": false,
     "title": ""
    },
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3caafc95-6776-46f1-8c7c-40dc8c0efa9e",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "3cf2dd993b5e4d3daecf61e4bab5a404",
      "087b76a8b7514269b1f0ab29b062e444",
      "35b0e8c26d6640e9bd0ed7b242a423d8",
      "54ad89e05fd74576b9b8b5b5a10eaf8d",
      "a41dc44766444a998bec2d777f249d23",
      "a069d2ab23824f29aa320ac256e2cfe9",
      "06e806c82c7b4cbea31c5358dd9c3434",
      "2e5087c76f98437cb5dc729230358cba",
      "036fc5746f43416db18c19ad8fd36677",
      "fdb1941405ed4e4aa06019933892deb3",
      "668d5377ca56426a99753867e6e24862"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "bce9db22-b022-4e43-de3f-c7ea4c9c3c4e"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7480624846244aa0a6fe052fce17db1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=100,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eead8d4-8179-4efb-96be-c123c14511db",
     "showTitle": false,
     "title": ""
    },
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "c73d8dfa-f4a1-4a01-a6dc-018bf82516a2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4080 Laptop GPU. Max memory = 11.994 GB.\n1.703 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5e3278a-4ea1-403d-b38b-8ca8da6592cb",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "69117b9b-b6f8-4d0e-c262-6998ba2c46bd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 4,528 | Num Epochs = 10\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 5,660\n \"-____-\"     Number of trainable parameters = 18,464,768\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5660' max='5660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5660/5660 2:17:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.404500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.264600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.733800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.122200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9a50b22-d47d-4313-b696-9fbdcf185a3b",
     "showTitle": false,
     "title": ""
    },
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "98f78253-86cf-4673-ff2b-923460c2b3fd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8228.0963 seconds used for training.\n137.13 minutes used for training.\nPeak reserved memory = 2.381 GB.\nPeak reserved memory for training = 0.678 GB.\nPeak reserved memory % of max memory = 19.852 %.\nPeak reserved memory for training % of max memory = 5.653 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b493ac75-a4d4-463f-bd42-1638d25f533a",
     "showTitle": false,
     "title": ""
    },
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, uploading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e147d8a3-ee9a-4ca6-900d-54fe38675150",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "bdd5b069-e944-4c81-8094-468999c210ec"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('models/Qwen2-1.5B-MAC-lora/tokenizer_config.json',\n",
       " 'models/Qwen2-1.5B-MAC-lora/special_tokens_map.json',\n",
       " 'models/Qwen2-1.5B-MAC-lora/vocab.json',\n",
       " 'models/Qwen2-1.5B-MAC-lora/merges.txt',\n",
       " 'models/Qwen2-1.5B-MAC-lora/added_tokens.json',\n",
       " 'models/Qwen2-1.5B-MAC-lora/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_lora_model = local_model + \"lora\"\n",
    "model.save_pretrained(local_lora_model)  # Local saving\n",
    "tokenizer.save_pretrained(local_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "695a7cfe-92e9-46f4-8f89-df0269fb4c99",
     "showTitle": false,
     "title": ""
    },
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e33a74-6380-4511-a7b4-51a4e07d68a2",
     "showTitle": false,
     "title": ""
    },
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3047e16b-6947-445e-bfe5-fbd67e41d3bc",
     "showTitle": false,
     "title": ""
    },
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        local_lora_model,  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82435d49-f297-497e-a801-5e92893226c1",
     "showTitle": false,
     "title": ""
    },
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10bf54d7-3af7-4eba-83e4-0ca950d60277",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, save_method, publish=True):\n",
    "    model.save_pretrained_merged(\n",
    "        local_model + save_method,\n",
    "        tokenizer,\n",
    "        save_method=save_method,\n",
    "    )\n",
    "\n",
    "    if publish:\n",
    "        model.push_to_hub_merged(\n",
    "            hub_model + save_method,\n",
    "            tokenizer,\n",
    "            save_method=save_method,\n",
    "            token=token,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07669ca0-fcaa-4bb9-863e-362e13d4926c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... Done.\nUnsloth: Saving LoRA adapters. Please wait...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b475732d1147758c43a50fda8c103d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/73.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lora model to https://huggingface.co/Qwen2-1.5B-MAC-lora\n"
     ]
    }
   ],
   "source": [
    "save_model(model, tokenizer, \"lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ce565a-3cbd-4efa-8e58-7ca7ed619df9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 4bit...\nThis might take 5 minutes...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/llm-fine-tune/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\nUnsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 10 minutes for Llama-7b... Done.\nUnsloth: Merging 4bit and LoRA weights to 4bit...\nThis might take 5 minutes...\nDone.\nUnsloth: Saving 4bit Bitsandbytes model. Please wait...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2203d4732724445a193160cbb78d90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged_4bit model to https://huggingface.co/Qwen2-1.5B-MAC-merged_4bit_forced\n"
     ]
    }
   ],
   "source": [
    "save_model(model, tokenizer, \"merged_4bit_forced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b04b1e1c-5a54-494a-a8a6-139429ec7cd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 27.21 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 203.78it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 27.15 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 89.17it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving to organization with address inflaton/Qwen2-1.5B-MAC-merged_16bit\nUnsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nUnsloth: Saving to organization with address inflaton/Qwen2-1.5B-MAC-merged_16bit\nUnsloth: Uploading all files... Please wait...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ce27d0c3ef4b1ba3bb6a6e01dc88d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818ba16c9bfe49f9bcc162b4ca67d1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc084f12fde4b45a5cc4e01dd469c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/727M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\nSaved merged model to https://huggingface.co/None/Qwen2-1.5B-MAC-merged_16bit\n"
     ]
    }
   ],
   "source": [
    "save_model(model, tokenizer, \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2034125-48cf-4464-b75b-f591483397ca",
     "showTitle": false,
     "title": ""
    },
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3561c025-73cc-4cb2-bd42-24eab8d0568c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_model_gguf(model, tokenizer, quantization_method, publish=True):\n",
    "    model.save_pretrained_gguf(\n",
    "        local_model + quantization_method,\n",
    "        tokenizer,\n",
    "        quantization_method=quantization_method,\n",
    "    )\n",
    "\n",
    "    if publish:\n",
    "        model.push_to_hub_gguf(\n",
    "            hub_model + \"gguf-\" + quantization_method,\n",
    "            tokenizer,\n",
    "            quantization_method=quantization_method,\n",
    "            token=token,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea512650-7e36-4c12-885e-be9a713e6228",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 30.75 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 99.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting qwen2 model. Can use fast conversion = False.\nUnsloth: We must use f16 for non Llama and Mistral models.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to f16 will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at models/Qwen2-1.5B-MAC-f16 into f16 GGUF format.\nThe output location will be ./models/Qwen2-1.5B-MAC-f16-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-MAC-f16\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'models/Qwen2-1.5B-MAC-f16-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  20%|‚ñà‚ñâ        | 615M/3.09G [00:01<00:04, 532Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 418Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'models/Qwen2-1.5B-MAC-f16-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-MAC-f16-unsloth.F16.gguf\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 30.74 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 131.88it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to f16 will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at Qwen2-1.5B-MAC-gguf-f16 into f16 GGUF format.\nThe output location will be ./Qwen2-1.5B-MAC-gguf-f16-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-MAC-gguf-f16\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'Qwen2-1.5B-MAC-gguf-f16-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  20%|‚ñà‚ñâ        | 615M/3.09G [00:01<00:04, 520Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 388Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'Qwen2-1.5B-MAC-gguf-f16-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-MAC-gguf-f16-unsloth.F16.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f319ee6aa55460b820a40536598edf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2-1.5B-MAC-gguf-f16-unsloth.F16.gguf:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/inflaton/Qwen2-1.5B-MAC-gguf-f16\n"
     ]
    }
   ],
   "source": [
    "save_model_gguf(model, tokenizer, quantization_method=\"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7e73bc9-54fe-43ea-a7c0-82fe9dc31371",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 30.75 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 107.87it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q8_0 will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at models/Qwen2-1.5B-MAC-q8_0 into f16 GGUF format.\nThe output location will be ./models/Qwen2-1.5B-MAC-q8_0-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-MAC-q8_0\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'models/Qwen2-1.5B-MAC-q8_0-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  21%|‚ñà‚ñà        | 643M/3.09G [00:02<00:08, 293Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:09<00:00, 319Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'models/Qwen2-1.5B-MAC-q8_0-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-MAC-q8_0-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './models/Qwen2-1.5B-MAC-q8_0-unsloth.F16.gguf' to './models/Qwen2-1.5B-MAC-q8_0-unsloth.Q8_0.gguf' as Q8_0 using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./models/Qwen2-1.5B-MAC-q8_0-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-MAC-q8_0\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q8_0 .. size =   445.12 MiB ->   236.47 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\nggml_validate_row_data: found inf value at block 196608\nllama_model_quantize: failed to quantize: tensor 'blk.1.attn_v.weight' has invalid data\nmain: failed to quantize model from './models/Qwen2-1.5B-MAC-q8_0-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-MAC-q8_0-unsloth.Q8_0.gguf\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 30.76 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 108.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q8_0 will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at Qwen2-1.5B-MAC-gguf-q8_0 into f16 GGUF format.\nThe output location will be ./Qwen2-1.5B-MAC-gguf-q8_0-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-MAC-gguf-q8_0\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'Qwen2-1.5B-MAC-gguf-q8_0-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  20%|‚ñà‚ñâ        | 615M/3.09G [00:01<00:04, 540Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 425Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'Qwen2-1.5B-MAC-gguf-q8_0-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-MAC-gguf-q8_0-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './Qwen2-1.5B-MAC-gguf-q8_0-unsloth.F16.gguf' to './Qwen2-1.5B-MAC-gguf-q8_0-unsloth.Q8_0.gguf' as Q8_0 using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./Qwen2-1.5B-MAC-gguf-q8_0-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-MAC-gguf-q8_0\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q8_0 .. size =   445.12 MiB ->   236.47 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\nggml_validate_row_data: found inf value at block 196608\nllama_model_quantize: failed to quantize: tensor 'blk.1.attn_v.weight' has invalid data\nmain: failed to quantize model from './Qwen2-1.5B-MAC-gguf-q8_0-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-MAC-gguf-q8_0-unsloth.Q8_0.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f01e4eb1e24beb898a1084d1948d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2-1.5B-MAC-gguf-q8_0-unsloth.Q8_0.gguf:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/inflaton/Qwen2-1.5B-MAC-gguf-q8_0\n"
     ]
    }
   ],
   "source": [
    "save_model_gguf(model, tokenizer, quantization_method=\"q8_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f0fa116-adae-47fc-a707-edfe268d0e75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 30.76 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 120.41it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q5_k_m will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at models/Qwen2-1.5B-MAC-q5_k_m into f16 GGUF format.\nThe output location will be ./models/Qwen2-1.5B-MAC-q5_k_m-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-MAC-q5_k_m\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'models/Qwen2-1.5B-MAC-q5_k_m-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  20%|‚ñà‚ñâ        | 615M/3.09G [00:01<00:04, 550Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 428Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'models/Qwen2-1.5B-MAC-q5_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-MAC-q5_k_m-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q5_k_m. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './models/Qwen2-1.5B-MAC-q5_k_m-unsloth.F16.gguf' to './models/Qwen2-1.5B-MAC-q5_k_m-unsloth.Q5_K_M.gguf' as Q5_K_M using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./models/Qwen2-1.5B-MAC-q5_k_m-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-MAC-q5_k_m\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\nggml_validate_row_data: found inf value at block 196608\nllama_model_quantize: failed to quantize: tensor 'blk.1.attn_v.weight' has invalid data\nmain: failed to quantize model from './models/Qwen2-1.5B-MAC-q5_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-MAC-q5_k_m-unsloth.Q5_K_M.gguf\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 30.75 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 95.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q5_k_m will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at Qwen2-1.5B-MAC-gguf-q5_k_m into f16 GGUF format.\nThe output location will be ./Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-MAC-gguf-q5_k_m\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  21%|‚ñà‚ñà        | 643M/3.09G [00:01<00:05, 414Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 404Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q5_k_m. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.F16.gguf' to './Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.Q5_K_M.gguf' as Q5_K_M using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-MAC-gguf-q5_k_m\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\nggml_validate_row_data: found inf value at block 196608\nllama_model_quantize: failed to quantize: tensor 'blk.1.attn_v.weight' has invalid data\nmain: failed to quantize model from './Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.Q5_K_M.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86280df7d2d74eda800155bca3814b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2-1.5B-MAC-gguf-q5_k_m-unsloth.Q5_K_M.gguf:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/inflaton/Qwen2-1.5B-MAC-gguf-q5_k_m\n"
     ]
    }
   ],
   "source": [
    "save_model_gguf(model, tokenizer, quantization_method=\"q5_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7158640a-294a-4fce-bbb4-b45494ef15a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 30.75 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 71.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at models/Qwen2-1.5B-MAC-q4_k_m into f16 GGUF format.\nThe output location will be ./models/Qwen2-1.5B-MAC-q4_k_m-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-MAC-q4_k_m\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'models/Qwen2-1.5B-MAC-q4_k_m-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  20%|‚ñà‚ñâ        | 615M/3.09G [00:01<00:04, 535Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 432Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'models/Qwen2-1.5B-MAC-q4_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-MAC-q4_k_m-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './models/Qwen2-1.5B-MAC-q4_k_m-unsloth.F16.gguf' to './models/Qwen2-1.5B-MAC-q4_k_m-unsloth.Q4_K_M.gguf' as Q4_K_M using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./models/Qwen2-1.5B-MAC-q4_k_m-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-MAC-q4_k_m\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\nggml_validate_row_data: found inf value at block 196608\nllama_model_quantize: failed to quantize: tensor 'blk.1.attn_v.weight' has invalid data\nmain: failed to quantize model from './models/Qwen2-1.5B-MAC-q4_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./models/Qwen2-1.5B-MAC-q4_k_m-unsloth.Q4_K_M.gguf\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 30.76 out of 47.05 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 93.61it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n \"-____-\"     In total, you will have to wait around 26 minutes.\n\nUnsloth: [0] Installing llama.cpp. This will take 3 minutes...\nUnsloth: [1] Converting model at Qwen2-1.5B-MAC-gguf-q4_k_m into f16 GGUF format.\nThe output location will be ./Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.F16.gguf\nThis will take 3 minutes...\nINFO:hf-to-gguf:Loading model: Qwen2-1.5B-MAC-gguf-q4_k_m\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 1536\nINFO:hf-to-gguf:gguf: feed forward length = 8960\nINFO:hf-to-gguf:gguf: head count = 12\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model tokenizer\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\nINFO:hf-to-gguf:Exporting model to 'Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.F16.gguf'\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8960, 1536}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {1536, 8960}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {1536, 1536}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {1536, 256}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\nWriting:  20%|‚ñà‚ñâ        | 615M/3.09G [00:01<00:04, 531Mbyte/s]/home/inflaton/code/projects/courses/cs605/project/llama.cpp/gguf-py/gguf/lazy.py:230: RuntimeWarning: overflow encountered in cast\n  return type(self)(meta=meta, args=full_args, lazy=self._lazy, func=(lambda a: a[0].astype(*a[1:], **kwargs)))\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.09G/3.09G [00:07<00:00, 424Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to 'Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\nmain: build = 3139 (bfaa676b)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing './Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.F16.gguf' to './Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.Q4_K_M.gguf' as Q4_K_M using 48 threads\nllama_model_loader: loaded meta data with 21 key-value pairs and 254 tensors from ./Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-1.5B-MAC-gguf-q4_k_m\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   57 tensors\nllama_model_loader: - type  f16:  197 tensors\n[   1/ 254]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n[   2/ 254]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   3/ 254]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[   4/ 254]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[   5/ 254]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[   6/ 254]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[   7/ 254]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[   8/ 254]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[   9/ 254]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  10/ 254]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n[  11/ 254]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  12/ 254]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n[  13/ 254]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  14/ 254]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n[  15/ 254]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n[  16/ 254]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n[  17/ 254]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n[  18/ 254]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\nggml_validate_row_data: found inf value at block 196608\nllama_model_quantize: failed to quantize: tensor 'blk.1.attn_v.weight' has invalid data\nmain: failed to quantize model from './Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.F16.gguf'\nUnsloth: Conversion completed! Output location: ./Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.Q4_K_M.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655304cb31f64363b44508597fdeb6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2-1.5B-MAC-gguf-q4_k_m-unsloth.Q4_K_M.gguf:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/inflaton/Qwen2-1.5B-MAC-gguf-q4_k_m\n"
     ]
    }
   ],
   "source": [
    "save_model_gguf(model, tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57630e62-5983-44b6-8ba2-83d4702a940e",
     "showTitle": false,
     "title": ""
    },
    "id": "bDp0zNpwe6U_"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba9d6285-9d96-4d29-be53-1d768e351999",
     "showTitle": false,
     "title": ""
    },
    "id": "Zt9CHJqO6p30"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
    "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
    "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
    "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
    "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
    "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
    "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
    "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
    "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "02_MAC_+_Qwen2-1.5B_Unsloth_2x_faster_finetuning",
   "widgets": {}
  },
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "036fc5746f43416db18c19ad8fd36677": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06e806c82c7b4cbea31c5358dd9c3434": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "087b76a8b7514269b1f0ab29b062e444": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a069d2ab23824f29aa320ac256e2cfe9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_06e806c82c7b4cbea31c5358dd9c3434",
      "value": "Map‚Äá(num_proc=2):‚Äá100%"
     }
    },
    "09b76013aa9e45efb6deb23a7a0d0925": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dea41c5260884aa6879b5e1d1697b14f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_89965917796a4f81b899fdc7685f33df",
      "value": "config.json:‚Äá100%"
     }
    },
    "0a92c56bfa134ef583220d7ef0b13e17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c34be936c8145d3ab41282f30a70713": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f8b6bfe16894500838793f2491d403f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "177c78fce95d4b4ab33057c5a048d693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f44c9ce1adf470cbb19784493ed209f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c34be936c8145d3ab41282f30a70713",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0a92c56bfa134ef583220d7ef0b13e17",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "201b59ccd9f845e197029b57e424aefc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2157f01726d748f8a9ae4a00664430da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21db8a77b00d4a4e82fdfa608657531f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26e4202cca81496a90d15a0dd4ca9cf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba90fdb8822d47dab7ba203bee297f37",
       "IPY_MODEL_61560ff6a36b44f4a9dfdae5c52791d4",
       "IPY_MODEL_95fbe66647904c06a20f640630d6dc0e"
      ],
      "layout": "IPY_MODEL_57182a263d324a3dbf1471c74290a0d5"
     }
    },
    "27155728b6b84cb199c91c940095d0a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b91feeed5464877991ac2c207aebe7c",
       "IPY_MODEL_cca8113c54c0495daedce1327bf9c68b",
       "IPY_MODEL_2e63a29e2f7247bba5beede9a568c99f"
      ],
      "layout": "IPY_MODEL_5c9d781c28944f3eb86e2a6d44efdf18"
     }
    },
    "271ddaa553a042d09b6db7b450643d8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a58d04b428c46f4b3dbadd3bc6cd529": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d18ddf6482c4d97829ac0e5a7b9868f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f679ad3ec7f4fe8ad0510ffb57bc2ab",
       "IPY_MODEL_f2df530d22c74977b249dd9fb5f4829b",
       "IPY_MODEL_89b2ef0dbfea47ab8e6f8d659e3351d1"
      ],
      "layout": "IPY_MODEL_3056b148aa9f4e6e8aa3b61d26886255"
     }
    },
    "2e5087c76f98437cb5dc729230358cba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e63a29e2f7247bba5beede9a568c99f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b993eaec6b224440bf80c0958c6fb536",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_de868e26e7154f62aa86223a539ad421",
      "value": "‚Äá464/464‚Äá[00:00&lt;00:00,‚Äá27.1kB/s]"
     }
    },
    "2f6c70dd266c4816bfad3fd3d192929a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30307300bc4e4baf96560e30969a82b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e36a3f9eff0e4cf68834d66b0213ae96",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a0037bdccf254159becde630bee3d1db",
      "value": "generation_config.json:‚Äá100%"
     }
    },
    "3056b148aa9f4e6e8aa3b61d26886255": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30cdc32298134cb0be4d41615b9e5774": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3572201bd4d74a58b7a665f9bdfdcdba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "35b0e8c26d6640e9bd0ed7b242a423d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e5087c76f98437cb5dc729230358cba",
      "max": 51760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_036fc5746f43416db18c19ad8fd36677",
      "value": 51760
     }
    },
    "36166c7bcb854b34aca1f41a5d6ea50b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "370692d819df41828b48c4ad446f977b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39b29a75374b45c0a22506010be2b84e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30cdc32298134cb0be4d41615b9e5774",
      "max": 1179,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47928317548c454bba6358ab132e8dee",
      "value": 1179
     }
    },
    "3cf2dd993b5e4d3daecf61e4bab5a404": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_087b76a8b7514269b1f0ab29b062e444",
       "IPY_MODEL_35b0e8c26d6640e9bd0ed7b242a423d8",
       "IPY_MODEL_54ad89e05fd74576b9b8b5b5a10eaf8d"
      ],
      "layout": "IPY_MODEL_a41dc44766444a998bec2d777f249d23"
     }
    },
    "43dec2ede91341f5af60eb522e18e984": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4463edd481c1467f914c7dcd6c6e6ffc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47928317548c454bba6358ab132e8dee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49277aeeac16434a865a4d12308b1abc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ae7e449e4ea4c729b5f34607c18ebae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b2061b8a73c43ffb0c2f83daf0d0183": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c4c88d4c701450692fa0f6b0c5764b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c666f4ace3943f8b80ecd20e7503236": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ccedf0d93094e63b57a0f8a434fba06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4463edd481c1467f914c7dcd6c6e6ffc",
      "max": 44307561,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d3b9a05db0b4dadb638c686faa0c40a",
      "value": 44307561
     }
    },
    "4dcf6ff672d24983a1877a8431709aa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5807d5fb827d490fb3bc698f801ffff5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c4f2b06a82fd4987b8b659524a7b503b",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
     }
    },
    "4ea63adfce694725bdba878aef709dd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5234566b1bfc4655b8d582ea5b46ed9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54ad89e05fd74576b9b8b5b5a10eaf8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdb1941405ed4e4aa06019933892deb3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_668d5377ca56426a99753867e6e24862",
      "value": "‚Äá51760/51760‚Äá[01:02&lt;00:00,‚Äá1131.51‚Äáexamples/s]"
     }
    },
    "56aee4853b7740e6a977254f5d1fa66d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57182a263d324a3dbf1471c74290a0d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5807d5fb827d490fb3bc698f801ffff5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c9d781c28944f3eb86e2a6d44efdf18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f40db8173dd4d76b6ef5ed6d9ec8b6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61560ff6a36b44f4a9dfdae5c52791d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db19fc8d37db4e45a5790a876836d8c4",
      "max": 11610,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36166c7bcb854b34aca1f41a5d6ea50b",
      "value": 11610
     }
    },
    "6578fd7acdb54c4c93528ea431fd0144": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_370692d819df41828b48c4ad446f977b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a0bf9160eb2647409b3200270914b90f",
      "value": "‚Äá50.6k/50.6k‚Äá[00:00&lt;00:00,‚Äá2.71MB/s]"
     }
    },
    "668d5377ca56426a99753867e6e24862": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "697f027529b54ee9956bae78a11e0611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69ac12aec0714318bf2c83d4f4e745f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b2012c3f88547af8884a9ea90e3164b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_938f45f1b3e24118b815d96ae34ba86a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9367047a800747f79c6b225d92397846",
      "value": "‚Äá44.3M/44.3M‚Äá[00:01&lt;00:00,‚Äá31.0MB/s]"
     }
    },
    "6b91feeed5464877991ac2c207aebe7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b2061b8a73c43ffb0c2f83daf0d0183",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_69ac12aec0714318bf2c83d4f4e745f5",
      "value": "special_tokens_map.json:‚Äá100%"
     }
    },
    "6d3b9a05db0b4dadb638c686faa0c40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6dbbedeca9314e66ae50e44ffa31a414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6e34619b45934040b6092e6fb01ea7fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71ce208e20d6483abb9ed923510c86d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d69dc491b3ab44d7852b21873ed7bb7f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f401d53bf28e44eb906bce6c05412662",
      "value": "‚Äá51760/51760‚Äá[00:01&lt;00:00,‚Äá45512.81‚Äáexamples/s]"
     }
    },
    "7358cdad832342c983e31efb8754ab78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73e352a3404f4c7dad0737f57d29e92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_988a0e8c1f89446086858da0a891a79c",
       "IPY_MODEL_4ccedf0d93094e63b57a0f8a434fba06",
       "IPY_MODEL_6b2012c3f88547af8884a9ea90e3164b"
      ],
      "layout": "IPY_MODEL_7e29cb8dd4df4d5b94407cd8fd3f2011"
     }
    },
    "74501720ac7e4dbb911a4a99b3633bc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78e5400bff924a92a4cc61c4ff18b182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9b313fd861948f5aba25b24b1518d30",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4c666f4ace3943f8b80ecd20e7503236",
      "value": "‚Äá1.18k/1.18k‚Äá[00:00&lt;00:00,‚Äá31.3kB/s]"
     }
    },
    "7975adbc2ec5489ea7fa0167e620d85c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e34619b45934040b6092e6fb01ea7fe",
      "max": 51760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_271ddaa553a042d09b6db7b450643d8f",
      "value": 51760
     }
    },
    "7e29cb8dd4df4d5b94407cd8fd3f2011": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "810ff6c0e17d4fa09a30fef27eacff90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "89965917796a4f81b899fdc7685f33df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89b2ef0dbfea47ab8e6f8d659e3351d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8908fa0df3743ecb9d12983a739104f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_177c78fce95d4b4ab33057c5a048d693",
      "value": "‚Äá9.09M/9.09M‚Äá[00:00&lt;00:00,‚Äá32.6MB/s]"
     }
    },
    "8b3505352a5a42bf910428c40ce40465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49277aeeac16434a865a4d12308b1abc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2157f01726d748f8a9ae4a00664430da",
      "value": "‚Äá5.70G/5.70G‚Äá[01:02&lt;00:00,‚Äá30.1MB/s]"
     }
    },
    "8fc142b628fb40568730234de1cafde2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ae7e449e4ea4c729b5f34607c18ebae",
      "max": 172,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3572201bd4d74a58b7a665f9bdfdcdba",
      "value": 172
     }
    },
    "9367047a800747f79c6b225d92397846": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "938f45f1b3e24118b815d96ae34ba86a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95fbe66647904c06a20f640630d6dc0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0a370dc20654b279b9680692e34418e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cfeb365ddf7548d58b2557f22737fcf5",
      "value": "‚Äá11.6k/11.6k‚Äá[00:00&lt;00:00,‚Äá716kB/s]"
     }
    },
    "988a0e8c1f89446086858da0a891a79c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad2be500fc164c0f86f33e914ef8e6a0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5234566b1bfc4655b8d582ea5b46ed9f",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "98c58f23f4d549518832cb2d18f796e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09b76013aa9e45efb6deb23a7a0d0925",
       "IPY_MODEL_39b29a75374b45c0a22506010be2b84e",
       "IPY_MODEL_78e5400bff924a92a4cc61c4ff18b182"
      ],
      "layout": "IPY_MODEL_2a58d04b428c46f4b3dbadd3bc6cd529"
     }
    },
    "99fdbb0300c14c139d1937c646f0cfe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7358cdad832342c983e31efb8754ab78",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e9adf418296e436fb48bb9f78885598b",
      "value": "‚Äá51760/51760‚Äá[00:01&lt;00:00,‚Äá38665.95‚Äáexamples/s]"
     }
    },
    "9f679ad3ec7f4fe8ad0510ffb57bc2ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ea63adfce694725bdba878aef709dd3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_74501720ac7e4dbb911a4a99b3633bc6",
      "value": "tokenizer.json:‚Äá100%"
     }
    },
    "a0037bdccf254159becde630bee3d1db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a069d2ab23824f29aa320ac256e2cfe9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0bf9160eb2647409b3200270914b90f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a41dc44766444a998bec2d777f249d23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8464a4c711e4e00aafdfc919b60d07e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb995c740590427b882572c81d4e848c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_201b59ccd9f845e197029b57e424aefc",
      "value": "‚Äá172/172‚Äá[00:00&lt;00:00,‚Äá12.0kB/s]"
     }
    },
    "a9f0cc51fc3d4d7b874c32dcf1c5bdf2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad2be500fc164c0f86f33e914ef8e6a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0240cd9a4554b29ae11f8051984a1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edaf890370314a218f138015faa0b05d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_697f027529b54ee9956bae78a11e0611",
      "value": "Map:‚Äá100%"
     }
    },
    "b0a370dc20654b279b9680692e34418e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b518dcee69074b87be73957cd810e7ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d891f8d0b1fc462f8008d02bb2a15692",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cced8fd7e998472794f3f3e3018956a5",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "b8908fa0df3743ecb9d12983a739104f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b993eaec6b224440bf80c0958c6fb536": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9b313fd861948f5aba25b24b1518d30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba90fdb8822d47dab7ba203bee297f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f8b6bfe16894500838793f2491d403f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bb19f6c747754682a514373a3a0535ba",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "bb19f6c747754682a514373a3a0535ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc883d4cf13e4f8b8a4fe5f410cb6efd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9159e03e61f4f56978ece9c3bca49b2",
      "max": 51760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_810ff6c0e17d4fa09a30fef27eacff90",
      "value": 51760
     }
    },
    "c161d94df0f04feba9542237e0856c22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c22f71b1f85843209d7e5321506b9cb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f44c9ce1adf470cbb19784493ed209f",
       "IPY_MODEL_f1addc4479d849879e743cf9089e6540",
       "IPY_MODEL_8b3505352a5a42bf910428c40ce40465"
      ],
      "layout": "IPY_MODEL_4c4c88d4c701450692fa0f6b0c5764b0"
     }
    },
    "c4f2b06a82fd4987b8b659524a7b503b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cca8113c54c0495daedce1327bf9c68b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e02f9b7849c64531835eb77b860d1c93",
      "max": 464,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_56aee4853b7740e6a977254f5d1fa66d",
      "value": 464
     }
    },
    "cced8fd7e998472794f3f3e3018956a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf245afeb1c04f29a24d291608c3d157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b518dcee69074b87be73957cd810e7ed",
       "IPY_MODEL_e29104486d594b2992d7285e0ef77371",
       "IPY_MODEL_6578fd7acdb54c4c93528ea431fd0144"
      ],
      "layout": "IPY_MODEL_d35db8148a354c56aaac56dbae22536f"
     }
    },
    "cfe8cae0e22b495bafa221a63d13b283": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfeb365ddf7548d58b2557f22737fcf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1b47d39450d4019ae85c9b2f943eeaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dcf6ff672d24983a1877a8431709aa9",
       "IPY_MODEL_7975adbc2ec5489ea7fa0167e620d85c",
       "IPY_MODEL_71ce208e20d6483abb9ed923510c86d7"
      ],
      "layout": "IPY_MODEL_cfe8cae0e22b495bafa221a63d13b283"
     }
    },
    "d35db8148a354c56aaac56dbae22536f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d69dc491b3ab44d7852b21873ed7bb7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d891f8d0b1fc462f8008d02bb2a15692": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8e5318cead340c4adbeaccc05d39225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "daf4cd890b35422683d22fd30bc71e83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0240cd9a4554b29ae11f8051984a1c6",
       "IPY_MODEL_bc883d4cf13e4f8b8a4fe5f410cb6efd",
       "IPY_MODEL_99fdbb0300c14c139d1937c646f0cfe7"
      ],
      "layout": "IPY_MODEL_c161d94df0f04feba9542237e0856c22"
     }
    },
    "db19fc8d37db4e45a5790a876836d8c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de868e26e7154f62aa86223a539ad421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dea41c5260884aa6879b5e1d1697b14f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e02f9b7849c64531835eb77b860d1c93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e29104486d594b2992d7285e0ef77371": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9f0cc51fc3d4d7b874c32dcf1c5bdf2",
      "max": 50641,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2f6c70dd266c4816bfad3fd3d192929a",
      "value": 50641
     }
    },
    "e36a3f9eff0e4cf68834d66b0213ae96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9159e03e61f4f56978ece9c3bca49b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9adf418296e436fb48bb9f78885598b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "edaf890370314a218f138015faa0b05d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1addc4479d849879e743cf9089e6540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43dec2ede91341f5af60eb522e18e984",
      "max": 5702746405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d8e5318cead340c4adbeaccc05d39225",
      "value": 5702746405
     }
    },
    "f2df530d22c74977b249dd9fb5f4829b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21db8a77b00d4a4e82fdfa608657531f",
      "max": 9085698,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6dbbedeca9314e66ae50e44ffa31a414",
      "value": 9085698
     }
    },
    "f401d53bf28e44eb906bce6c05412662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb995c740590427b882572c81d4e848c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fce7a61c25ec4390af43d92b7c473a45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_30307300bc4e4baf96560e30969a82b6",
       "IPY_MODEL_8fc142b628fb40568730234de1cafde2",
       "IPY_MODEL_a8464a4c711e4e00aafdfc919b60d07e"
      ],
      "layout": "IPY_MODEL_5f40db8173dd4d76b6ef5ed6d9ec8b6e"
     }
    },
    "fdb1941405ed4e4aa06019933892deb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
